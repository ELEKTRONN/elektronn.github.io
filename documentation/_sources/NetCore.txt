*********************************
Tips for Adaptation and Extending
*********************************

General
-------

Many customisations can be achieved by sub-classing objects from the :py:mod:`training` package or adding new functions in the layers, e.g.:
	- If you wish to create another network architecture or use layer options which are not configurable through the config file (e.g. input noise or weight sharing between layers) you can still use the pipeline but make a sub-class of :py:class:`training.trainer.Trainer` in which the ``createNet`` method is overridden by your own code and everything else is inherited. Then you must only make sure that in the script ``TrainCNN.py`` your own Trainer class is used.
	- If you wish another weight initialisation, you can override the ``randomizeWeights`` methods in the layer classes (e.g. there is already fragmentary code for creating gabor filters for 2d conv layers)


Watch Outs
----------

	- In the training pipeline, it is not possible to have all imports at the top of the file because some imports can only be made after some conditions are fulfilled (e.g. previous to any theano imports the device must be initialised - otherwise the value from ``.theanorc`` is used and it cannot be changed later on)
	- When adding new configuration options to the pipeline, you must also put them into ``training/config.py`` otherwise they will be considered as invalid/non-existing parameters.
	- For 3d data the internal order is (z,x,y) for performance reasons, but filter shapes etc. are defined as (x,y,z) on the frontend and swapped internally. The full internal processing order for 3d convolutions is (batch, z, channel, x, y). This might be a bit confusing in the implementation.

