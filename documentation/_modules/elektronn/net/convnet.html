<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>elektronn.net.convnet &mdash; ELEKTRONN</title>
    
    <link rel="stylesheet" href="../../../_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '0.1rc',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../../../_static/elektronnfavicon.ico"/>
    <link rel="top" title="ELEKTRONN" href="../../../index.html" />
    <link rel="up" title="Module code" href="../../index.html" /> 
  </head>
  <body role="document">

<div style="background-color: white; text-align: left; padding: 10px 10px 15px 15px">
<a href="../../../index.html"><img src="../../../_static/elektronn.png" border="0" alt="sampledoc"/></a>
</div>

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">ELEKTRONN</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" accesskey="U">Module code</a> &raquo;</li> 
      </ul>
    </div>

      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for elektronn.net.convnet</h1><div class="highlight"><pre>
<span class="c"># -*- coding: utf-8 -*-</span>
<span class="c"># ELEKTRONN - Neural Network Toolkit</span>
<span class="c">#</span>
<span class="c"># Copyright (c) 2014 - now</span>
<span class="c"># Max-Planck-Institute for Medical Research, Heidelberg, Germany</span>
<span class="c"># Authors: Marius Killinger, Gregor Urban</span>

<span class="k">print</span> <span class="s">&quot;Load ELEKTRONN Core&quot;</span>

<span class="kn">import</span> <span class="nn">time</span><span class="o">,</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">cPickle</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">T</span>

<span class="kn">from</span> <span class="nn">elektronn.utils</span> <span class="kn">import</span> <span class="n">pprinttime</span>

<span class="kn">import</span> <span class="nn">optimizer</span> <span class="kn">as</span> <span class="nn">opt</span>
<span class="kn">from</span> <span class="nn">perceptronlayer</span> <span class="kn">import</span> <span class="n">PerceptronLayer</span><span class="p">,</span> <span class="n">RecurrentLayer</span>
<span class="kn">from</span> <span class="nn">convlayer2d</span> <span class="kn">import</span> <span class="n">ConvLayer2d</span>
<span class="kn">from</span> <span class="nn">convlayer3d</span> <span class="kn">import</span> <span class="n">ConvLayer3d</span><span class="p">,</span> <span class="n">AffinityLayer3d</span><span class="p">,</span> <span class="n">MalisLayer</span>


<span class="k">def</span> <span class="nf">_printOps</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return a humanized string representation of a large number.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">abbrevs</span> <span class="o">=</span> <span class="p">((</span><span class="mi">1000000000000</span><span class="p">,</span> <span class="s">&#39;Tera Ops&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mi">1000000000</span><span class="p">,</span> <span class="s">&#39;Giga Ops&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mi">1000000</span><span class="p">,</span> <span class="s">&#39;Mega Ops&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="s">&#39;kilo Ops&#39;</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">factor</span><span class="p">,</span> <span class="n">suffix</span> <span class="ow">in</span> <span class="n">abbrevs</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">n</span> <span class="o">&gt;=</span> <span class="n">factor</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">print</span> <span class="s">&#39;Computational Cost: </span><span class="si">%.1f</span><span class="s"> </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">/</span> <span class="n">factor</span><span class="p">,</span> <span class="n">suffix</span><span class="p">)</span>


<div class="viewcode-block" id="MixedConvNN"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN">[docs]</a><span class="k">class</span> <span class="nc">MixedConvNN</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    input_size: tuple</span>
<span class="sd">    Data shapes, excluding batch and channel (used to infer the dimensionality)</span>
<span class="sd">    input_depth: int/None</span>
<span class="sd">    Is None by default this means non-image data (no conv layers allowed). Change to 1 for b/w, 3 for RGB and\</span>
<span class="sd">    4 for RGB-D images etc. For RNN this is the length of the time series.</span>
<span class="sd">    batch_size: int/None</span>
<span class="sd">    None for variable batch size</span>
<span class="sd">    enable_dropout:  Bool</span>
<span class="sd">    Turn on or off dropout</span>
<span class="sd">    recurrent:       Bool</span>
<span class="sd">    Support recurrent iterations along input depth/time</span>
<span class="sd">    dimension_calc: dimension calculator object</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>

<span class="sd">    Note that image data must have at least 1 channel, e.g. a 2d image (1,x,y). 3d requires data in the</span>
<span class="sd">    format (z,ch,x,y). E.g. to create an isotropic 3d CNN with 5 channels (total input shape is (1,30,5,30,30)):</span>

<span class="sd">    &gt;&gt;&gt; MixedConvNN((30,30,30), input_depth=5, batch_size=1)</span>

<span class="sd">    A non-convolutional MLP can be created as:</span>

<span class="sd">    &gt;&gt;&gt; MixedConvNN((100,), input_depth=None, batch_size=2000)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">input_size</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">input_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">enable_dropout</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                 <span class="n">recurrent</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                 <span class="n">dimension_calc</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">input_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c"># [0] input layer ---&gt; [-1] output layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">poolings</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_output_layers</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c"># Empty UNLESS you use add sth explicitly</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_autoencoder_chains</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_grads</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">debug_functions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">debug_conv_output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">debug_gradients_function</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">CG_timeline</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_lab</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_size</span>
                                   <span class="p">)</span>  <span class="c"># onlt the spatial part of input shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_strides</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mfp_strides</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mfp_offsets</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dimension_calc</span> <span class="o">=</span> <span class="n">dimension_calc</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">TotalForwardPassCost</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c"># number of multiplications done</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">SGD_LR</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.09</span><span class="p">))</span>  <span class="c"># those 3 values are to be overwritten</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">SGD_momentum</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.9</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_weightdecay</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_SGD_params</span> <span class="o">=</span> <span class="p">{</span><span class="s">&#39;LR&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">SGD_LR</span><span class="p">,</span> <span class="s">&#39;momentum&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">SGD_momentum</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_RPROP_params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_CG_params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_LBFGS_params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_class_weights</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_enable_dropout</span> <span class="o">=</span> <span class="n">enable_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_recurrent</span> <span class="o">=</span> <span class="n">recurrent</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_atleast_single_mfp</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_y_aux</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_noise</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_init</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">input_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">input_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="p">)</span>
        <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="k">assert</span> <span class="n">input_dim</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span>
        <span class="p">],</span> <span class="s">&quot;MixedConvNN: input_dimension currently not supported&quot;</span>

        <span class="k">if</span> <span class="n">input_dim</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">input_depth</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">input_depth</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">print</span> <span class="s">&quot;For image-like data no depth was specified, using depth=1&quot;</span>

        <span class="k">if</span> <span class="n">recurrent</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">input_dim</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">ftensor3</span><span class="p">(</span><span class="s">&#39;x_rnn_input&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">input_depth</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">input_depth</span><span class="p">,</span> <span class="n">input_size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c"># [batch, time, feat]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c"># the input is repeated (see &quot;iterations&quot; in rnn layer)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x_dim</span> <span class="o">=</span> <span class="n">input_dim</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c"># +1 because of leading batch dimension</span>
            <span class="k">if</span> <span class="n">input_depth</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>  <span class="c"># For images there is always an additional channel dimension (even if it is 1)</span>
                <span class="n">x_dim</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">input_depth</span><span class="p">)</span> <span class="o">+</span> <span class="n">input_size</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c"># For non image input / prohibits ConvLayers</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="p">)</span> <span class="o">+</span> <span class="n">input_size</span>

            <span class="c"># construct tensor of matching dimensionality</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="s">&#39;float32&#39;</span><span class="p">,</span> <span class="p">(</span><span class="bp">False</span><span class="p">,)</span> <span class="o">*</span> <span class="n">x_dim</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;x_cnn_input&#39;</span><span class="p">)()</span>
            <span class="k">if</span> <span class="n">input_dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>  <span class="c"># strange order for theano 3dconv</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_depth</span><span class="p">,</span> <span class="n">input_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_size</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

        <span class="k">print</span> <span class="s">&#39;-&#39;</span> <span class="o">*</span> <span class="mi">60</span>
        <span class="k">print</span> <span class="s">&quot;Input shape   = &quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="p">,</span> <span class="s">&quot;; This is a&quot;</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="s">&quot;dimensional NN&quot;</span>
        <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_layer0_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_layer0_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x</span>

        <span class="k">print</span> <span class="s">&#39;---&#39;</span>

    <span class="c">############################################################################################################</span>

<div class="viewcode-block" id="MixedConvNN.addPerceptronLayer"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.addPerceptronLayer">[docs]</a>    <span class="k">def</span> <span class="nf">addPerceptronLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">n_outputs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                           <span class="n">activation_func</span><span class="o">=</span><span class="s">&#39;tanh&#39;</span><span class="p">,</span>
                           <span class="n">enable_input_noise</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                           <span class="n">add_in_output_layers</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                           <span class="n">force_no_dropout</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                           <span class="n">W</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                           <span class="n">b</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adds a Perceptron layer to the CNN.</span>

<span class="sd">        Normally the each layer creates its own set of randomly initialised neuron weights. To reuse the weights</span>
<span class="sd">        of another layer (weight sharing) use the arguments ``W`` and ``b`` an pass ``T.TensorVariable``.</span>
<span class="sd">        If ``W`` and ``b`` are numpy arrays own weights are initialised with these values.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        n_outputs: int</span>
<span class="sd">          The size of this layer</span>
<span class="sd">        activation_func: string</span>
<span class="sd">          {tanh, relu, sigmoid, abs, linear, maxout &lt;i&gt;}</span>
<span class="sd">          Activation function</span>
<span class="sd">        enable_input_noise: Bool</span>
<span class="sd">          If True set 20% of input to 0 randomly (similar to dropout)</span>
<span class="sd">        force_no_dropout:   Bool</span>
<span class="sd">          Set True for last/output layer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">layer_input_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">==</span> <span class="p">[])</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output_shape</span>
        <span class="n">layer_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer0_input</span> <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">==</span> <span class="p">[])</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_input_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>  <span class="c"># input_dimension &gt;= 2</span>
            <span class="n">layer_input</span> <span class="o">=</span> <span class="n">layer_input</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">nin</span> <span class="o">=</span> <span class="p">(</span><span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>  <span class="c"># input_dimension = 1</span>
            <span class="n">nin</span> <span class="o">=</span> <span class="n">layer_input_shape</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;Used invalid input dimension for Perceptron layer&#39;</span><span class="p">)</span>

        <span class="n">input_noise</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span> <span class="k">if</span> <span class="n">enable_input_noise</span> <span class="k">else</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_noise</span> <span class="o">=</span> <span class="n">input_noise</span> <span class="k">if</span> <span class="n">enable_input_noise</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_noise</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">wvector</span><span class="p">(</span><span class="s">&#39;y_cnn_labels&#39;</span><span class="p">)</span>
        <span class="n">layer</span> <span class="o">=</span> <span class="n">PerceptronLayer</span><span class="p">(</span>
            <span class="nb">input</span><span class="o">=</span><span class="n">layer_input</span><span class="p">,</span>
            <span class="n">n_in</span><span class="o">=</span><span class="n">nin</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">n_out</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">nin</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">enable_dropout</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_enable_dropout</span> <span class="ow">and</span> <span class="n">force_no_dropout</span> <span class="o">==</span> <span class="bp">False</span><span class="p">),</span>
            <span class="n">activation_func</span><span class="o">=</span><span class="n">activation_func</span><span class="p">,</span>
            <span class="n">input_noise</span><span class="o">=</span><span class="n">input_noise</span><span class="p">,</span>
            <span class="n">input_layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
            <span class="n">W</span><span class="o">=</span><span class="n">W</span><span class="p">,</span>
            <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">add_in_output_layers</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_output_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">num_multiplications</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">n_outputs</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">layer_input_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">num_multiplications</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">n_outputs</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
        <span class="n">_printOps</span><span class="p">(</span><span class="n">num_multiplications</span><span class="p">)</span>
        <span class="k">print</span> <span class="s">&#39;---&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">TotalForwardPassCost</span> <span class="o">+=</span> <span class="n">num_multiplications</span>

    <span class="c">############################################################################################################</span>
</div>
<div class="viewcode-block" id="MixedConvNN.addConvLayer"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.addConvLayer">[docs]</a>    <span class="k">def</span> <span class="nf">addConvLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">nof_filters</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                     <span class="n">filter_size</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                     <span class="n">pool_shape</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                     <span class="n">activation_func</span><span class="o">=</span><span class="s">&#39;tanh&#39;</span><span class="p">,</span>
                     <span class="n">add_in_output_layers</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                     <span class="n">force_no_dropout</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                     <span class="n">use_fragment_pooling</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                     <span class="n">reshape</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                     <span class="n">is_last_layer</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                     <span class="n">layer_input_shape</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                     <span class="n">layer_input</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                     <span class="n">W</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                     <span class="n">b</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                     <span class="n">pooling_mode</span><span class="o">=</span><span class="s">&#39;max&#39;</span><span class="p">,</span>
                     <span class="n">affinity</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adds a convolutional layer to the CNN. The dimensionality is *automatically* inferred.</span>

<span class="sd">        Normally the inputs are automatically connected the the outputs of the last added layer. To connect to a</span>
<span class="sd">        different layer use ``layer_input_shape`` and ``layer_input`` arguments.</span>

<span class="sd">        Normally the each layer creates its own set of randomly initialised neuron weights. To reuse the weights</span>
<span class="sd">        of another layer (weight sharing) use the arguments ``W`` and ``b`` an pass ``T.TensorVariable``.</span>
<span class="sd">        If ``W`` and ``b`` are numpy arrays own weights are initialised with these values.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        nof_filters:          int</span>
<span class="sd">          Number of feature maps</span>
<span class="sd">        filter_size:          int/tuple</span>
<span class="sd">          Size/shape of convolutional filters, xy/zxy, (scalars are automatically extended to the 2d or 3d)</span>
<span class="sd">        pool_shape:   int/tuple</span>
<span class="sd">          Size/shape of pool, xy/zxy, (scalars are automatically extended to the 2d or 3d)</span>
<span class="sd">        activation_func:      string</span>
<span class="sd">          {tanh, relu, sigmoid, abs, linear, maxout &lt;i&gt;}</span>
<span class="sd">          Activation function</span>
<span class="sd">        force_no_dropout:     Bool</span>
<span class="sd">          Set True for last/output layer</span>
<span class="sd">        use_fragment_pooling: Bool</span>
<span class="sd">         Set to True for predicting dense images efficiently. Requires batch_size==1.</span>
<span class="sd">        reshape:              Bool</span>
<span class="sd">          Set to True to get 2d/3d output instead of flattened class_probabilities in the last layer</span>
<span class="sd">        is_last_layer:        Bool</span>
<span class="sd">          Shorthand for reshape=True, force_no_dropout=True and reconstruction of pooling fragments (if mfp was active)</span>
<span class="sd">        layer_input_shape: tuple of int</span>
<span class="sd">          Only needed if layer_input is not not None</span>
<span class="sd">        layer_input: T.TensorVariable</span>
<span class="sd">          Symbolic input if you do *not* want to use the previous layer of the cnn. This requires specification of</span>
<span class="sd">          the shape of that input with ``layer_input_shape``.</span>
<span class="sd">        W: np.ndarray</span>
<span class="sd">          weight matrix. If array, the values are used to initialise a shared variable for this layer.</span>
<span class="sd">                               If TensorVariable, than this variable is directly used (weight sharing with the</span>
<span class="sd">                               layer from which this variable comes from)</span>

<span class="sd">        b: np.ndarray or T.TensorVariable</span>
<span class="sd">          bias vector. If array, the values are used to initialise a shared variable for this layer.</span>
<span class="sd">                               If TensorVariable, than this variable is directly used (weight sharing with the</span>
<span class="sd">                               layer from which this variable comes from)</span>
<span class="sd">        pooling_mode: str</span>
<span class="sd">          &#39;max&#39; or &#39;maxabs&#39; where the first is normal maxpooling and the second also retains sign of large negative values</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">n_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span>
        <span class="k">assert</span> <span class="n">n_dim</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="s">&quot;only 2d and 3d convolution supported!&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">filter_size</span><span class="p">,</span> <span class="s">&#39;__len__&#39;</span><span class="p">):</span>
            <span class="n">filter_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">filter_size</span><span class="p">,</span> <span class="p">)</span> <span class="o">*</span> <span class="n">n_dim</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">filter_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">filter_size</span> <span class="o">=</span> <span class="n">filter_size</span> <span class="o">*</span> <span class="n">n_dim</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">filter_size</span><span class="p">)</span> <span class="o">!=</span> <span class="n">n_dim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s">&#39;Filter size must be either scalar or have same length as n_dim&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">pool_shape</span><span class="p">,</span> <span class="s">&#39;__len__&#39;</span><span class="p">):</span>
            <span class="n">pool_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">pool_shape</span><span class="p">,</span> <span class="p">)</span> <span class="o">*</span> <span class="n">n_dim</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">pool_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">pool_shape</span> <span class="o">=</span> <span class="n">pool_shape</span> <span class="o">*</span> <span class="n">n_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">poolings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pool_shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">layer_input_shape</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">layer_input</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">):</span>
            <span class="n">layer_input_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output_shape</span>
            <span class="n">layer_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer0_input</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span><span class="n">layer_input_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">layer_input</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">),</span>\
                <span class="s">&quot;Provide either both input and shape or neither&quot;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_input_shape</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span>\
            <span class="s">&quot;Please implement the stacking of a convLayer on top of PerceptronLayer (if this is your goal)&quot;</span>

        <span class="k">if</span> <span class="n">is_last_layer</span><span class="p">:</span>
            <span class="k">print</span> <span class="s">&quot;Last Layer, by default: no dropout and reshaped outputs&quot;</span>
            <span class="n">force_no_dropout</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="n">reshape</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_atleast_single_mfp</span><span class="p">:</span>
                <span class="n">use_fragment_pooling</span> <span class="o">=</span> <span class="bp">True</span>

        <span class="k">if</span> <span class="n">use_fragment_pooling</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s">&quot;MFP is activated and batch_size is not 1&quot;</span><span class="p">)</span>
                <span class="c">#raise ValueError(&quot;MFP is activated and batch_size is not 1&quot;)</span>
                <span class="c"># self.batch_size = 1 doesn&#39;t help</span>

                <span class="c"># if there is mfp in at least 1 layer the output must be reshaped</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_atleast_single_mfp</span> <span class="o">=</span> <span class="n">use_fragment_pooling</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_atleast_single_mfp</span>

        <span class="n">dropout</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_enable_dropout</span> <span class="ow">and</span> <span class="n">force_no_dropout</span> <span class="o">==</span> <span class="bp">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">filter_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">nof_filters</span><span class="p">,</span> <span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">filter_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">filter_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">CL</span> <span class="o">=</span> <span class="n">ConvLayer2d</span>
            <span class="k">if</span> <span class="n">reshape</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="s">&#39;int16&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">False</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">False</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;y_cnn_labels&#39;</span><span class="p">)()</span>

        <span class="k">if</span> <span class="n">n_dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">filter_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">nof_filters</span><span class="p">,</span> <span class="n">filter_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">filter_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">filter_size</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">CL</span> <span class="o">=</span> <span class="n">ConvLayer3d</span>

            <span class="k">if</span> <span class="n">affinity</span><span class="p">:</span>
                <span class="k">print</span> <span class="s">&quot;WARNING: hack for adding affinity layer / MALIS active&quot;</span>
                <span class="k">if</span> <span class="n">affinity</span> <span class="o">==</span> <span class="s">&#39;malis&#39;</span><span class="p">:</span>
                    <span class="n">CL</span> <span class="o">=</span> <span class="n">MalisLayer</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">CL</span> <span class="o">=</span> <span class="n">AffinityLayer3d</span>

            <span class="k">if</span> <span class="n">reshape</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="s">&#39;int16&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">False</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">False</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;y_cnn_labels&#39;</span><span class="p">)()</span>

        <span class="n">layer</span> <span class="o">=</span> <span class="n">CL</span><span class="p">(</span>
            <span class="n">layer_input</span><span class="p">,</span>
            <span class="n">layer_input_shape</span><span class="p">,</span>
            <span class="n">filter_shape</span><span class="p">,</span>
            <span class="n">pool_shape</span><span class="p">,</span>
            <span class="n">activation_func</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
            <span class="n">use_fragment_pooling</span><span class="p">,</span>
            <span class="n">reshape</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mfp_offsets</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mfp_strides</span><span class="p">,</span>
            <span class="n">input_layer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
            <span class="n">W</span><span class="o">=</span><span class="n">W</span><span class="p">,</span>
            <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span>
            <span class="n">pooling_mode</span><span class="o">=</span><span class="n">pooling_mode</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mfp_offsets</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">mfp_offsets</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mfp_strides</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">mfp_strides</span>

        <span class="k">if</span> <span class="n">add_in_output_layers</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_output_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>

        <span class="c"># Calculate computational cost</span>
        <span class="k">if</span> <span class="n">n_dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">n_pos</span> <span class="o">=</span> <span class="p">((</span><span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="o">-</span><span class="n">filter_size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span>\
                     <span class="p">(</span><span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="o">-</span><span class="n">filter_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">if</span> <span class="n">n_dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">n_pos</span> <span class="o">=</span> <span class="p">((</span><span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="o">-</span><span class="n">filter_size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span>\
                     <span class="p">(</span><span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="o">-</span><span class="n">filter_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span>\
                     <span class="p">(</span><span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="o">-</span><span class="n">filter_size</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">num_multiplications</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">filter_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_pos</span> <span class="o">*</span> <span class="n">nof_filters</span> <span class="o">*</span>\
                                  <span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">n_dim</span><span class="o">==</span><span class="mi">2</span> <span class="k">else</span> <span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">num_multiplications</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">filter_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_pos</span> <span class="o">*</span> <span class="n">nof_filters</span> <span class="o">*</span>\
                                  <span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">n_dim</span><span class="o">==</span><span class="mi">2</span> <span class="k">else</span> <span class="mi">2</span><span class="p">]</span> <span class="c"># Cost for 1 patch</span>

        <span class="n">_printOps</span><span class="p">(</span><span class="n">num_multiplications</span><span class="p">)</span>
        <span class="k">print</span> <span class="s">&quot;Param count:&quot;</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_value</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="s">&#39;+&#39;</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_value</span><span class="p">()</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="s">&#39;=&#39;</span><span class="p">,</span>\
              <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_value</span><span class="p">()</span><span class="o">.</span><span class="n">size</span> <span class="o">+</span> <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_value</span><span class="p">()</span><span class="o">.</span><span class="n">size</span>
        <span class="k">print</span> <span class="s">&#39;---&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">TotalForwardPassCost</span> <span class="o">+=</span> <span class="n">num_multiplications</span>

    <span class="c">############################################################################################################</span>
</div>
<div class="viewcode-block" id="MixedConvNN.addRecurrentLayer"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.addRecurrentLayer">[docs]</a>    <span class="k">def</span> <span class="nf">addRecurrentLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                          <span class="n">n_hid</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                          <span class="n">activation_func</span><span class="o">=</span><span class="s">&#39;tanh&#39;</span><span class="p">,</span>
                          <span class="n">iterations</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adds a recurrent layer (only possible for non-image input of format (batch, time, features))</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        n_hid: int</span>
<span class="sd">           Number of hidden units</span>
<span class="sd">        activation_func: string</span>
<span class="sd">          {tanh, relu, sigmoid, abs, linear}</span>
<span class="sd">        iterations: int</span>
<span class="sd">          If layer input is not time-like (iterable on axis 1) it can be broadcasted and</span>
<span class="sd">          iterated over for a fixed number of iterations</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">layer_input_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span> <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">==</span> <span class="p">[])</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output_shape</span>
        <span class="n">layer_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer0_input</span> <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">==</span> <span class="p">[])</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span>
        <span class="c"># Padding of constant input</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">print</span> <span class="s">&quot;Recurrence with broadcasted input&quot;</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">iterations</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
            <span class="n">bs</span> <span class="o">=</span> <span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="p">(</span><span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">)</span> <span class="k">else</span> <span class="mi">1</span>
            <span class="n">broadcaster</span> <span class="o">=</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">layer_input</span> <span class="o">=</span> <span class="n">layer_input</span><span class="o">.</span><span class="n">dimshuffle</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s">&#39;x&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">broadcaster</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">)</span>
            <span class="n">layer_input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">iterations</span><span class="p">,</span> <span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;Used invalid input dimension for Recurrent layer&#39;</span><span class="p">)</span>

        <span class="n">nin</span> <span class="o">=</span> <span class="n">layer_input_shape</span>  <span class="c"># [batch, time, features]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">wvector</span><span class="p">(</span><span class="s">&#39;y_cnn_labels&#39;</span><span class="p">)</span>
        <span class="n">layer</span> <span class="o">=</span> <span class="n">RecurrentLayer</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">layer_input</span><span class="p">,</span>
                               <span class="n">n_in</span><span class="o">=</span><span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                               <span class="n">n_hid</span><span class="o">=</span><span class="n">n_hid</span><span class="p">,</span>
                               <span class="n">batch_size</span><span class="o">=</span><span class="n">layer_input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                               <span class="n">activation_func</span><span class="o">=</span><span class="n">activation_func</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">num_multiplications</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">nin</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_hid</span> <span class="o">+</span> <span class="n">nin</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">nin</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_hid</span><span class="o">**</span><span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">num_multiplications</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">nin</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">*</span> <span class="n">n_hid</span> <span class="o">+</span> <span class="n">nin</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_hid</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">_printOps</span><span class="p">(</span><span class="n">num_multiplications</span><span class="p">)</span>
        <span class="k">print</span> <span class="s">&#39;---&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">TotalForwardPassCost</span> <span class="o">+=</span> <span class="n">num_multiplications</span>

    <span class="c">############################################################################################################</span>
</div>
<div class="viewcode-block" id="MixedConvNN.addTiedAutoencoderChain"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.addTiedAutoencoderChain">[docs]</a>    <span class="k">def</span> <span class="nf">addTiedAutoencoderChain</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                <span class="n">n_layers</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                                <span class="n">force_no_dropout</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                <span class="n">activation_func</span><span class="o">=</span><span class="s">&#39;tanh&#39;</span><span class="p">,</span>
                                <span class="n">input_noise</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                                <span class="n">tie_W</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates connected layers to invert Perceptron layers. Input is assumed to come from the first layer.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        n_layers: int</span>
<span class="sd">           Number of layers that will be added/inverted, (input &lt; 0 means all)</span>
<span class="sd">        activation_func:      string</span>
<span class="sd">           {tanh, relu, sigmoid, abs, linear}</span>
<span class="sd">           Activation function</span>
<span class="sd">        force_no_dropout:     Bool</span>
<span class="sd">          set True for last/output layer</span>
<span class="sd">        input_noise:         Bool</span>
<span class="sd">          Noise rate that will be applied to the input of the first reconstructor</span>
<span class="sd">        tie_W:                Bool</span>
<span class="sd">          Whether to share weight of dual layer pairs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">n_layers</span><span class="p">:</span>  <span class="c"># Automatically find number of Layers if not specified</span>
            <span class="n">n_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>
        <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">n_layers</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">),</span> <span class="s">&quot;Number of Autoencoder layers not possible&quot;</span>

        <span class="n">chain</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]]</span>  <span class="c"># if n_layers = depth(NN), add last layer, if n_layers is smaller add</span>
                                             <span class="c"># &lt;n_layers&gt;th layer (s.t. a MLP remains after the AE bzw. next to it))</span>

        <span class="n">first</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>  <span class="c"># Invert layers starting from the deepest layer</span>
            <span class="n">n_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">n_in</span>  <span class="c"># Get n_out and Weights from mirror layer</span>
            <span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span> <span class="k">if</span> <span class="n">tie_W</span> <span class="k">else</span> <span class="bp">None</span>
            <span class="n">n_inputs</span> <span class="o">=</span> <span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c"># Get input from previous layer in chain</span>
            <span class="c"># (the first in chain is the deepest layer in the normal Net)</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">dropout</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enable_dropout</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">force_no_dropout</span>
            <span class="n">noise</span> <span class="o">=</span> <span class="n">input_noise</span> <span class="k">if</span> <span class="n">first</span> <span class="k">else</span> <span class="bp">None</span>
            <span class="n">PLayer</span> <span class="o">=</span> <span class="n">PerceptronLayer</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">,</span>
                                     <span class="n">n_in</span><span class="o">=</span><span class="n">n_inputs</span><span class="p">,</span>
                                     <span class="n">n_out</span><span class="o">=</span><span class="n">n_outputs</span><span class="p">,</span>
                                     <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                     <span class="n">enable_dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
                                     <span class="n">activation_func</span><span class="o">=</span><span class="n">activation_func</span><span class="p">,</span>
                                     <span class="n">W</span><span class="o">=</span><span class="n">W</span><span class="p">,</span>
                                     <span class="n">input_noise</span><span class="o">=</span><span class="n">noise</span><span class="p">,</span>
                                     <span class="n">input_layer</span><span class="o">=</span><span class="n">chain</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">chain</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">PLayer</span><span class="p">)</span>
            <span class="n">first</span> <span class="o">=</span> <span class="bp">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_autoencoder_chains</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">chain</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>  <span class="c"># only keep the newly added Layers</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">tie_W</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">+=</span> <span class="n">chain</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="c">############################################################################################################</span>
</div>
<div class="viewcode-block" id="MixedConvNN.compileDebugFunctions"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.compileDebugFunctions">[docs]</a>    <span class="k">def</span> <span class="nf">compileDebugFunctions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradients</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compiles the debug_functions which return the network activations / output. To use them compile them with</span>
<span class="sd">        this function. They by accessible as cnn.debug_functions (normal output), cnn.debug_conv_output,</span>
<span class="sd">        cnn.debug_gradients_function (if True).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">debug_functions</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span> <span class="s">&quot;debug functions are not empty&quot;</span>
            <span class="k">return</span>

        <span class="k">for</span> <span class="n">lay</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">debug_functions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_x</span><span class="p">],</span> <span class="n">lay</span><span class="o">.</span><span class="n">output</span><span class="p">))</span>
            <span class="k">try</span><span class="p">:</span>  <span class="c"># This is the output before pooling etc.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">debug_conv_output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
                    <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_x</span><span class="p">],</span>
                    <span class="n">lay</span><span class="o">.</span><span class="n">conv_output</span><span class="p">,</span>
                    <span class="n">on_unused_input</span><span class="o">=</span><span class="s">&#39;ignore&#39;</span><span class="p">))</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">pass</span>
        <span class="k">if</span> <span class="n">gradients</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">debug_gradients_function</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">compileGradients</span><span class="p">()</span>

    <span class="c">############################################################################################################</span>
</div>
<div class="viewcode-block" id="MixedConvNN.compileOutputFunctions"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.compileOutputFunctions">[docs]</a>    <span class="k">def</span> <span class="nf">compileOutputFunctions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                               <span class="n">target</span><span class="o">=</span><span class="s">&#39;nll&#39;</span><span class="p">,</span>
                               <span class="n">use_class_weights</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                               <span class="n">use_example_weights</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                               <span class="n">use_lazy_labels</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                               <span class="n">use_label_prop</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                               <span class="n">only_forward</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compiles the output functions ``get_loss``, ``get_error``, ``class_probabilities`` and defines the</span>
<span class="sd">        gradient (which is not compiled)</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        target: string</span>
<span class="sd">         &#39;nll&#39;/&#39;regression&#39;, regression has squared error and nll_masked allows training with</span>
<span class="sd">          lazy labels; this requires the auxiliary (*aux) masks.</span>
<span class="sd">        use_class_weights: Bool</span>
<span class="sd">          whether to use class weights for the error</span>
<span class="sd">        use_example_weights: Bool</span>
<span class="sd">          whether to use example weights for the error</span>
<span class="sd">        use_lazy_labels: Bool</span>
<span class="sd">          whether to use lazy labels; this requires the auxiliary (*aux) masks</span>
<span class="sd">        use_label_prop: Bool</span>
<span class="sd">          whether to activate label propagation on unlabelled (-1) examples</span>
<span class="sd">        only_forwad: Bool</span>
<span class="sd">          This exlcudes the building of the gradient (faster)</span>

<span class="sd">        Defined functions:</span>

<span class="sd">        (They are accessible as methods of ``MixedConvNN``)</span>

<span class="sd">        get_loss: theano-function</span>
<span class="sd">          [data, labels(, *aux)] --&gt; [loss, loss_instance]</span>
<span class="sd">        get_error: theano-function</span>
<span class="sd">          [data, labels(, *aux)] --&gt; [loss, (error,) prediction] no error for regression</span>
<span class="sd">        class_probabilities: theano-function</span>
<span class="sd">          [data] --&gt; [prediction]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">print</span> <span class="s">&quot;GLOBAL&quot;</span>
        <span class="n">_printOps</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">TotalForwardPassCost</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_graph</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">lay</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">lay</span><span class="o">.</span><span class="n">params</span> <span class="o">!=</span> <span class="p">[]:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">lay</span><span class="o">.</span><span class="n">params</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c"># (b, W)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_autoencoder_chains</span> <span class="ow">is</span> <span class="ow">not</span> <span class="p">[]:</span>  <span class="c"># add thos layers, but not to the params</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_autoencoder_chains</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">param_count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">get_value</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">])</span>

        <span class="k">print</span> <span class="s">&quot;Total Count of trainable Parameters:&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_count</span>
        <span class="k">print</span> <span class="s">&quot;Building Computational Graph took </span><span class="si">%.3f</span><span class="s"> s&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">t_graph</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_init</span><span class="p">)</span>
        <span class="n">pp_cw</span> <span class="o">=</span> <span class="s">&quot;using class_weights&quot;</span> <span class="k">if</span> <span class="n">use_class_weights</span> <span class="k">else</span> <span class="s">&quot;using no class_weights&quot;</span>
        <span class="n">pp_ew</span> <span class="o">=</span> <span class="s">&quot;using example_weights&quot;</span> <span class="k">if</span> <span class="n">use_example_weights</span> <span class="k">else</span> <span class="s">&quot;using no example_weights&quot;</span>
        <span class="n">pp_ll</span> <span class="o">=</span> <span class="s">&quot;using lazy_labels&quot;</span> <span class="k">if</span> <span class="n">use_lazy_labels</span> <span class="k">else</span> <span class="s">&quot;using no lazy_labels&quot;</span>
        <span class="n">pp_lp</span> <span class="o">=</span> <span class="s">&quot;label propagation active&quot;</span> <span class="k">if</span> <span class="n">use_label_prop</span> <span class="k">else</span> <span class="s">&quot;label propagation inactive&quot;</span>
        <span class="k">print</span> <span class="s">&quot;Compiling output functions for </span><span class="si">%s</span><span class="s"> target:</span><span class="se">\n\t</span><span class="si">%s</span><span class="se">\n</span><span class="s"> </span><span class="se">\t</span><span class="si">%s</span><span class="se">\n</span><span class="s"> </span><span class="se">\t</span><span class="si">%s</span><span class="se">\n</span><span class="s"> </span><span class="se">\t</span><span class="si">%s</span><span class="se">\n</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span>
            <span class="n">target</span><span class="p">,</span> <span class="n">pp_cw</span><span class="p">,</span> <span class="n">pp_ew</span><span class="p">,</span> <span class="n">pp_ll</span><span class="p">,</span> <span class="n">pp_lp</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_layers</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span> <span class="s">&quot;Warning: &lt;compileOutputFunctions&gt; only applies to the LAST layer in self.layers </span><span class="se">\</span>
<span class="s">        (and ignores elements of self._output_layers)&quot;</span>

        <span class="n">layer_last</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span> <span class="o">=</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">output_shape</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layer_last</span><span class="o">.</span><span class="n">output_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>  <span class="c">#Perceptron layer or any other</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_lab</span> <span class="o">=</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_lab</span> <span class="o">=</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="c"># Define Target functions</span>
        <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s">&#39;regression&#39;</span><span class="p">:</span>
            <span class="n">n_dim_regression</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_last</span><span class="o">.</span><span class="n">output_shape</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="s">&#39;float32&#39;</span><span class="p">,</span> <span class="p">(</span><span class="bp">False</span><span class="p">,)</span> <span class="o">*</span> <span class="n">n_dim_regression</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;y_cnn_regression_targets&#39;</span><span class="p">)()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_instance</span> <span class="o">=</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">squared_distance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">)</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">,</span> <span class="n">T</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">),</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">output</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">get_error</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">],</span> <span class="n">ret</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prediction</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_x</span><span class="p">],</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">target</span> <span class="o">==</span> <span class="s">&#39;nll_mutiple_binary&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">wmatrix</span><span class="p">(</span><span class="s">&#39;y_nll_mutiple_binary_targets&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">use_class_weights</span><span class="p">:</span>
                <span class="n">class_weights</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="s">&#39;float32&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">False</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;class_weights&#39;</span><span class="p">)()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_y_aux</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">class_weights</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">class_weights</span> <span class="o">=</span> <span class="bp">None</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_instance</span> <span class="o">=</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">nll_mutiple_binary</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="n">class_weights</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">target</span> <span class="o">==</span> <span class="s">&#39;nll_weak&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_class_weights</span><span class="p">:</span>
                <span class="n">class_weights</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="s">&#39;float32&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">False</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;class_weights&#39;</span><span class="p">)()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_y_aux</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">class_weights</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">class_weights</span> <span class="o">=</span> <span class="bp">None</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_instance</span> <span class="o">=</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">NLL_weak</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="n">class_weights</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">target</span> <span class="o">==</span> <span class="s">&#39;affinity&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="s">&#39;int16&#39;</span><span class="p">,</span> <span class="p">(</span><span class="bp">False</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;y_cnn_affinity_targets&#39;</span><span class="p">)()</span>
            <span class="k">if</span> <span class="n">use_class_weights</span><span class="p">:</span>
                <span class="n">class_weights</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="s">&#39;float32&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">False</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;class_weights&#39;</span><span class="p">)()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_y_aux</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">class_weights</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">class_weights</span> <span class="o">=</span> <span class="bp">None</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_instance</span> <span class="o">=</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">NLL_affinity</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="n">class_weights</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">target</span> <span class="o">==</span> <span class="s">&#39;malis&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="s">&#39;int16&#39;</span><span class="p">,</span> <span class="p">(</span><span class="bp">False</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;y_cnn_affinity_targets&#39;</span><span class="p">)()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_y_aux</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="s">&#39;int16&#39;</span><span class="p">,</span> <span class="p">(</span><span class="bp">False</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;y_cnn_seg_gt&#39;</span><span class="p">)())</span>
            <span class="k">if</span> <span class="n">use_class_weights</span><span class="p">:</span>
                <span class="n">class_weights</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="s">&#39;float32&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">False</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;class_weights&#39;</span><span class="p">)()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_y_aux</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">class_weights</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">class_weights</span> <span class="o">=</span> <span class="bp">None</span>

            <span class="n">ret</span> <span class="o">=</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">NLL_Malis</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_aux</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span> <span class="o">=</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_loss_instance</span> <span class="o">=</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">malis_stats</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_aux</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">ret</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">target</span> <span class="o">==</span> <span class="s">&#39;nll&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">use_lazy_labels</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_last</span><span class="p">,</span> <span class="n">ConvLayer2d</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_last</span><span class="p">,</span> <span class="n">ConvLayer3d</span><span class="p">)):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&quot;Cannot use lazy labels for Percptron layer&quot;</span><span class="p">)</span>

                <span class="n">mask1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="s">&#39;int16&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">False</span><span class="p">,</span> <span class="bp">False</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;mask_class_labeled&#39;</span><span class="p">)()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_y_aux</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask1</span><span class="p">)</span>
                <span class="n">mask2</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="s">&#39;int16&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">False</span><span class="p">,</span> <span class="bp">False</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;mask_class_not_present&#39;</span><span class="p">)()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_y_aux</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask2</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mask1</span><span class="p">,</span> <span class="n">mask2</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>

            <span class="k">if</span> <span class="n">use_class_weights</span><span class="p">:</span>
                <span class="n">class_weights</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="s">&#39;float32&#39;</span><span class="p">,</span> <span class="p">[</span><span class="bp">False</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;class_weights&#39;</span><span class="p">)()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_y_aux</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">class_weights</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">class_weights</span> <span class="o">=</span> <span class="bp">None</span>

            <span class="k">if</span> <span class="n">use_example_weights</span><span class="p">:</span>
                <span class="n">example_weights</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="s">&#39;float32&#39;</span><span class="p">,</span> <span class="p">(</span><span class="bp">False</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;example_weights&#39;</span><span class="p">)()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_y_aux</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">example_weights</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">example_weights</span> <span class="o">=</span> <span class="bp">None</span>

            <span class="k">if</span> <span class="n">use_label_prop</span><span class="p">:</span>
                <span class="n">label_prop_thresh</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">fscalar</span><span class="p">(</span><span class="s">&#39;label_prop_thresh&#39;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_y_aux</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label_prop_thresh</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">label_prop_thresh</span> <span class="o">=</span> <span class="bp">None</span>

            <span class="k">if</span> <span class="n">use_lazy_labels</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_instance</span> <span class="o">=</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">NLL</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span>
                    <span class="n">class_weights</span><span class="p">,</span>
                    <span class="n">example_weights</span><span class="p">,</span>
                    <span class="n">mask_class_labeled</span><span class="o">=</span><span class="n">mask1</span><span class="p">,</span>
                    <span class="n">mask_class_not_present</span><span class="o">=</span><span class="n">mask2</span><span class="p">,</span>
                    <span class="n">label_prop_thresh</span><span class="o">=</span><span class="n">label_prop_thresh</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_instance</span> <span class="o">=</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">NLL</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">,</span>
                    <span class="n">class_weights</span><span class="p">,</span>
                    <span class="n">example_weights</span><span class="p">,</span>
                    <span class="n">label_prop_thresh</span><span class="o">=</span><span class="n">label_prop_thresh</span><span class="p">)</span>

                <span class="c"># aux is possibly [mask_class_labeled, mask_class_not_present, class_weights, label_prop_thresh]</span>

                <span class="c"># For all targets except regression the predictions / accuracy</span>
        <span class="k">if</span> <span class="n">target</span> <span class="o">!=</span> <span class="s">&#39;regression&#39;</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">,</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">errors</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">),</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">class_prediction</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s">&#39;nll_mutiple_binary&#39;</span><span class="p">:</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">,</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">errors_no_tn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">),</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">class_prediction</span><span class="p">]</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">get_error</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_aux</span><span class="p">,</span> <span class="n">ret</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">class_probabilities</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_x</span><span class="p">],</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">class_probabilities</span><span class="p">)</span>

        <span class="c"># create a list of symbolic gradients for all model parameters</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">only_forward</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_gradients</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">disconnected_inputs</span><span class="o">=</span><span class="s">&quot;warn&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">get_loss</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer_last</span><span class="p">,</span> <span class="p">(</span><span class="n">ConvLayer2d</span><span class="p">,</span> <span class="n">ConvLayer3d</span><span class="p">,</span> <span class="n">AffinityLayer3d</span><span class="p">)):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span> <span class="o">=</span> <span class="n">layer_last</span><span class="o">.</span><span class="n">prob_shape</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">pass</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">output_strides</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">,</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">poolings</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mfp_strides</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output_strides</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mfp_strides</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t_out</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">print</span> <span class="s">&quot; Compiling done  - in </span><span class="si">%.3f</span><span class="s"> s!&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">t_out</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_graph</span><span class="p">)</span>
        <span class="k">print</span> <span class="s">&#39;-&#39;</span> <span class="o">*</span> <span class="mi">60</span>
        <span class="k">print</span> <span class="s">&#39;-&#39;</span> <span class="o">*</span> <span class="mi">60</span>

    <span class="c">############################################################################################################</span>
</div>
<div class="viewcode-block" id="MixedConvNN.resetMomenta"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.resetMomenta">[docs]</a>    <span class="k">def</span> <span class="nf">resetMomenta</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Resets the trailing average of the gradient to sole current gradient&quot;&quot;&quot;</span>
        <span class="k">print</span> <span class="s">&quot;CNN: resetting momenta&quot;</span>
        <span class="k">print</span> <span class="s">&#39;</span><span class="se">\t</span><span class="s">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_grads</span><span class="p">)])</span>
        <span class="k">for</span> <span class="n">para</span><span class="p">,</span> <span class="n">lg</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_grads</span><span class="p">):</span>
            <span class="n">sp</span> <span class="o">=</span> <span class="n">para</span><span class="o">.</span><span class="n">get_value</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">lg</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">sp</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">),</span> <span class="n">borrow</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">para</span><span class="p">,</span> <span class="n">rp</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_RPROP_LRs</span><span class="p">,</span> <span class="p">):</span>
                <span class="n">sp</span> <span class="o">=</span> <span class="n">para</span><span class="o">.</span><span class="n">get_value</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span>
                <span class="n">rp</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="mf">1e-3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">sp</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">),</span> <span class="n">borrow</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">pass</span>
</div>
<div class="viewcode-block" id="MixedConvNN.randomizeWeights"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.randomizeWeights">[docs]</a>    <span class="k">def</span> <span class="nf">randomizeWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reset_momenta</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Resets weights to random values (calls randomize_weights() on each layer)&quot;&quot;&quot;</span>
        <span class="k">print</span> <span class="s">&quot;CNN: Randomizing weights&quot;</span>
        <span class="k">for</span> <span class="n">lay</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_layers</span><span class="p">:</span>
            <span class="n">lay</span><span class="o">.</span><span class="n">randomizeWeights</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">reset_momenta</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">resetMomenta</span><span class="p">()</span>

    <span class="c">############################################################################################################</span>
    <span class="c">### Controlling Training ###################################################################################</span>
    <span class="c">############################################################################################################</span>
</div>
<div class="viewcode-block" id="MixedConvNN.trainingStep"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.trainingStep">[docs]</a>    <span class="k">def</span> <span class="nf">trainingStep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform one optimiser iteration.</span>
<span class="sd">        Optimizers can be chosen by the kwarg ``mode``. They are complied on demand (which may take a while) and cached</span>

<span class="sd">        **Signature**: cnn.trainingStep(data, label(, *aux)(,**kwargs))</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        data: float32 array</span>
<span class="sd">          input [bs, ch (, x, y)] or [bs, z, ch, x, y]</span>
<span class="sd">        labels: int16 array</span>
<span class="sd">          [bs,((z,)y,x)] if output is not flattened</span>
<span class="sd">        aux: int16 arrays</span>
<span class="sd">          (optional) auxiliary weights/masks/etc. Should be unpacked list</span>
<span class="sd">        kwargs:</span>
<span class="sd">          * mode: string</span>
<span class="sd">              [&#39;SGD&#39;]: (default) Good if data set is big and redundant</span>

<span class="sd">              &#39;RPROP&#39;: which does neither uses a fix learning rate nor the momentum-value.</span>
<span class="sd">              It is faster than SGD if you do full-batch Training and use NO dropout.</span>
<span class="sd">              Any source of noise leads to failure of convergence (at all).</span>

<span class="sd">              &#39;CG&#39;: Good generalisation but requires large batches. Returns current loss always</span>

<span class="sd">              &#39;LBFGS&#39;: http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html</span>


<span class="sd">           * update_loss: Bool</span>
<span class="sd">               determine current loss *after* update step (e.g. needed for queue, but ``get_loss`` can also be\</span>
<span class="sd">               called explicitly)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss: float32</span>
<span class="sd">          loss (nll or squared error)</span>
<span class="sd">        loss_instance: float32 array</span>
<span class="sd">          loss for individual batch examples/pixels</span>
<span class="sd">        time_per_step: float</span>
<span class="sd">          Time spent on the GPU per step</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;mode&#39;</span><span class="p">,</span> <span class="s">&#39;SGD&#39;</span><span class="p">)</span>
        <span class="n">param_var</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="c"># Check if auxiliary arguments are ok</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">!=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y_aux</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&quot;The number of auxiliary arguments for the NLL is not matching the compiled signature: &quot;</span>
                             <span class="s">&quot;</span><span class="si">%s</span><span class="s">. Got </span><span class="si">%i</span><span class="s"> auxiliary args.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_y_aux</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s">&#39;SGD&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&#39;SGD&#39;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">SGD</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">compileSGD</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_SGD_params</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

            <span class="n">loss</span><span class="p">,</span> <span class="n">loss_instance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;update_loss&#39;</span><span class="p">,</span> <span class="bp">False</span><span class="p">):</span>
                <span class="n">loss</span><span class="p">,</span> <span class="n">loss_instance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s">&#39;RPROP&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&#39;RPROP&#39;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">RPROP</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">compileRPROP</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_RPROP_params</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

            <span class="n">loss</span><span class="p">,</span> <span class="n">loss_instance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">RPROP</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;update_loss&#39;</span><span class="p">,</span> <span class="bp">False</span><span class="p">):</span>
                <span class="n">loss</span><span class="p">,</span> <span class="n">loss_instance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s">&#39;CG&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&#39;CG&#39;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">CG</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">compileCG</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_CG_params</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

            <span class="n">loss</span><span class="p">,</span> <span class="n">loss_instance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">CG</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>  <span class="c"># this already is updated loss</span>

        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s">&#39;LBFGS&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&#39;LBFGS&#39;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">LBFGS</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">compileLBFGS</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_LBFGS_params</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">LBFGS</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>  <span class="c"># this already is updated loss</span>
            <span class="n">loss_instance</span> <span class="o">=</span> <span class="n">loss</span>

        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s">&#39;Adam&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&#39;Adam&#39;</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">Adam</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">compileAdam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_Adam_params</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

            <span class="n">loss</span><span class="p">,</span> <span class="n">loss_instance</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>  <span class="c"># this already is updated loss</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span> <span class="s">&quot;No mode </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">mode</span>
            <span class="k">return</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>

        <span class="n">t</span> <span class="o">=</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span>  <span class="c"># add some epsilon to ensure &gt; 0</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">loss</span><span class="p">),</span> <span class="n">loss_instance</span><span class="p">,</span> <span class="n">t</span>  <span class="c">### TODO remove again</span>
</div>
<div class="viewcode-block" id="MixedConvNN.setOptimizerParams"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.setOptimizerParams">[docs]</a>    <span class="k">def</span> <span class="nf">setOptimizerParams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">SGD</span><span class="o">=</span><span class="p">{},</span>
                           <span class="n">CG</span><span class="o">=</span><span class="p">{},</span>
                           <span class="n">RPROP</span><span class="o">=</span><span class="p">{},</span>
                           <span class="n">LBFGS</span><span class="o">=</span><span class="p">{},</span>
                           <span class="n">Adam</span><span class="o">=</span><span class="p">{},</span>
                           <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialise optimiser hyper-parameters prior to compilation. SGD, CG and LBFGS this can also be done during</span>
<span class="sd">        Training.</span>

<span class="sd">        ``weight_decay`` is global to all optimisers and</span>
<span class="sd">        is identical to a L2-penalty on the weights with the coefficient given by ``weight_decay``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">==</span> <span class="bp">False</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">global_weightdecay</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">global_weightdecay</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">weight_decay</span><span class="p">),</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">setSGDLR</span><span class="p">(</span><span class="n">SGD</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;LR&quot;</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setSGDMomentum</span><span class="p">(</span><span class="n">SGD</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;momentum&quot;</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_RPROP_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="mf">0.35</span><span class="p">,</span>
                                  <span class="n">gain</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
                                  <span class="n">beta</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                                  <span class="n">initial_update_size</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_RPROP_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">RPROP</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_CG_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                               <span class="n">alpha</span><span class="o">=</span><span class="mf">0.35</span><span class="p">,</span>
                               <span class="n">beta</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                               <span class="n">max_step</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
                               <span class="n">min_step</span><span class="o">=</span><span class="mf">8e-5</span><span class="p">,</span>
                               <span class="n">only_descent</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                               <span class="n">show</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_CG_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">CG</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_LBFGS_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">maxfun</span><span class="o">=</span> <span class="mi">40</span><span class="p">,</span>  <span class="c"># function evaluations</span>
                                  <span class="n">maxiter</span><span class="o">=</span> <span class="mi">4</span><span class="p">,</span>  <span class="c"># iterations</span>
                                  <span class="n">m</span><span class="o">=</span> <span class="mi">10</span><span class="p">,</span>       <span class="c"># maximum number of variable metric corrections</span>
                                  <span class="n">factr</span><span class="o">=</span> <span class="mf">1e2</span><span class="p">,</span>  <span class="c"># factor of machine precision as termination criterion (haha!)</span>
                                  <span class="n">pgtol</span><span class="o">=</span> <span class="mf">1e-9</span><span class="p">,</span> <span class="c"># projected gradient tolerance</span>
                                  <span class="n">iprint</span><span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c"># set to 0 for direct printing of steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_LBFGS_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">LBFGS</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_Adam_params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_Adam_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">Adam</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&#39;SSGD&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">SSGD</span><span class="o">.</span><span class="n">updateOptimizerParams</span><span class="p">(</span><span class="n">SSGD</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&#39;CG&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">CG</span><span class="o">.</span><span class="n">updateOptimizerParams</span><span class="p">(</span><span class="n">CG</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s">&#39;LBFGS&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">LBFGS</span><span class="o">.</span><span class="n">updateOptimizerParams</span><span class="p">(</span><span class="n">LBFGS</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="MixedConvNN.setSGDLR"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.setSGDLR">[docs]</a>    <span class="k">def</span> <span class="nf">setSGDLR</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.09</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">SGD_LR</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">value</span><span class="p">),</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="MixedConvNN.setSGDMomentum"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.setSGDMomentum">[docs]</a>    <span class="k">def</span> <span class="nf">setSGDMomentum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">SGD_momentum</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">value</span><span class="p">),</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="MixedConvNN.setWeightDecay"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.setWeightDecay">[docs]</a>    <span class="k">def</span> <span class="nf">setWeightDecay</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_weightdecay</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">value</span><span class="p">),</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="MixedConvNN.setDropoutRates"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.setDropoutRates">[docs]</a>    <span class="k">def</span> <span class="nf">setDropoutRates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rates</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Assumes a vector/list/array as input, first entry &lt;--&gt; first layer (etc.)&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">lay</span><span class="p">,</span> <span class="n">ra</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">,</span> <span class="n">rates</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rates</span><span class="p">))):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">assert</span> <span class="mf">1.0</span> <span class="o">&gt;=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">ra</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s">&quot;Dropout rates must be [0,1]&quot;</span>
                <span class="n">lay</span><span class="o">.</span><span class="n">activation_noise</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">ra</span><span class="p">))</span>
                <span class="c">#print &#39;layer&#39;,i,&#39;new noise rate:&#39;,np.float32(ra*100.0),&#39;%&#39;</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="c">#print &#39;set_dropout_rates: Warning: Dropout not enabled in this layer&#39;</span>
                <span class="k">pass</span>
</div>
<div class="viewcode-block" id="MixedConvNN.getDropoutRates"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.getDropoutRates">[docs]</a>    <span class="k">def</span> <span class="nf">getDropoutRates</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns list of dropout rates&quot;&quot;&quot;</span>
        <span class="n">rates</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">lay</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">rates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">lay</span><span class="o">.</span><span class="n">activation_noise</span><span class="o">.</span><span class="n">get_value</span><span class="p">()))</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">pass</span>
                <span class="c">#sys.excepthook(*sys.exc_info())</span>
        <span class="k">return</span> <span class="n">rates</span>

    <span class="c">############################################################################################################</span>
    <span class="c">### Utilities ##############################################################################################</span>
    <span class="c">############################################################################################################</span>
</div>
    <span class="k">def</span> <span class="nf">_predictDenseTile</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_img</span><span class="p">,</span> <span class="n">out_arr</span><span class="p">,</span> <span class="n">offset</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        raw_img: np.ndarray</span>
<span class="sd">          raw image (ch, x, y) or (z, ch, x, y)to be predicted</span>
<span class="sd">          The shape must be cnn.patch_size + cnn.output_strides - 1 (elwise)</span>
<span class="sd">        out_arr: np.ndarray</span>
<span class="sd">          The shape is cnn.patch_size + cnn.mfp_strides - floor(cnn.offset) - 1 (elwise)</span>
<span class="sd">        offsets: array / list</span>
<span class="sd">          The cnn offsets (only needed if cnn was initialised without a dimension calculator)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        class_probabilities: np.ndarray</span>
<span class="sd">          prediction (n_lab, z, x, y)</span>
<span class="sd">          The shape is cnn.patch_size + cnn.mfp_strides - floor(cnn.offset) - 1 (elwise)</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_strides</span><span class="p">,</span> <span class="mi">1</span><span class="p">)):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">out_arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_probabilities</span><span class="p">(</span><span class="n">raw_img</span><span class="p">[</span><span class="bp">None</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>  <span class="c"># (ch,x,y)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out_arr</span><span class="p">[:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_probabilities</span><span class="p">(</span><span class="n">raw_img</span><span class="p">[</span><span class="bp">None</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>  <span class="c"># (z,ch,x,y)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">x_off</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_strides</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]):</span>
                <span class="k">for</span> <span class="n">y_off</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_strides</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                        <span class="n">cut_img</span> <span class="o">=</span> <span class="n">raw_img</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:,</span> <span class="n">x_off</span><span class="p">:</span><span class="n">x_off</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_off</span><span class="p">:</span><span class="n">y_off</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>

                        <span class="c">#prob = self.class_probabilities(cut_img)[0]</span>
                        <span class="c"># insert prob(ch, x, y) into out_arr(ch,z,x,y)</span>
                        <span class="n">out_arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x_off</span><span class="p">::</span><span class="bp">self</span><span class="o">.</span><span class="n">output_strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_off</span><span class="p">::</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">output_strides</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_probabilities</span><span class="p">(</span><span class="n">cut_img</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

                    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                        <span class="k">for</span> <span class="n">z_off</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_strides</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                            <span class="n">cut_img</span> <span class="o">=</span> <span class="n">raw_img</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span><span class="n">z_off</span><span class="p">:</span><span class="n">z_off</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">:,</span>
                                              <span class="n">x_off</span><span class="p">:</span><span class="n">x_off</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_off</span><span class="p">:</span><span class="n">y_off</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>

                            <span class="c">#prob = self.class_probabilities(cut_img)[0]</span>
                            <span class="n">out_arr</span><span class="p">[:,</span> <span class="n">z_off</span><span class="p">::</span><span class="bp">self</span><span class="o">.</span><span class="n">output_strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_off</span><span class="p">::</span><span class="bp">self</span><span class="o">.</span><span class="n">output_strides</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_off</span><span class="p">::</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">output_strides</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
                                   <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_probabilities</span><span class="p">(</span><span class="n">cut_img</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">out_arr</span>

<div class="viewcode-block" id="MixedConvNN.predictDense"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.predictDense">[docs]</a>    <span class="k">def</span> <span class="nf">predictDense</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">raw_img</span><span class="p">,</span>
                     <span class="n">show_progress</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                     <span class="n">offset</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                     <span class="n">as_uint8</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                     <span class="n">pad_raw</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Core function that performs the inference</span>

<span class="sd">        raw_img : np.ndarray</span>
<span class="sd">          raw data in the format (ch, x, y(, z))</span>
<span class="sd">        show_progress: Bool</span>
<span class="sd">          Whether to print progress state</span>
<span class="sd">        offset: 2/3-tuple</span>
<span class="sd">          If the cnn has no dimension calculator object, this specifies the cnn offset.</span>
<span class="sd">        as_uint8: Bool</span>
<span class="sd">          Return class proabilites as uint8 image (scaled between 0 and 255!)</span>
<span class="sd">        pad_raw: Bool</span>
<span class="sd">          Whether to apply padding (by mirroring) to the raw input image</span>
<span class="sd">          in order to get predictions on the full imgae domain.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c"># WARNING: this code contains mixed orders of xyz and zxy! The raw_img is swapped later!</span>

        <span class="c"># determine normalisation depending on int or float type</span>
        <span class="k">if</span> <span class="n">raw_img</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">,</span>
                             <span class="n">np</span><span class="o">.</span><span class="n">uint</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">]:</span>
            <span class="n">m</span> <span class="o">=</span> <span class="mi">255</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">m</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">raw_img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">raw_img</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="n">m</span>

        <span class="n">time_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">strip_z</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">raw_img</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">strip_z</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="n">raw_img</span> <span class="o">=</span> <span class="n">raw_img</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>  <span class="c"># add singleton z-channel</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimension_calc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">offset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dimension_calc</span><span class="o">.</span><span class="n">offset</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">offset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">,</span><span class="s">&quot;If the cnn has not been intialised with a dimension calculator object, you must pass the offset to this function explicitly&quot;</span>

            <span class="n">offset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">offset</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>

        <span class="n">n_lab</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_lab</span>
        <span class="n">cnn_out_sh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>  <span class="c"># without batch size and channel/n_lab</span>
        <span class="n">ps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span>
        <span class="n">strides</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_strides</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">cnn_out_sh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="p">],</span> <span class="n">cnn_out_sh</span><span class="p">])</span>
            <span class="n">ps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="p">],</span> <span class="n">ps</span><span class="p">])</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="p">],</span> <span class="n">strides</span><span class="p">])</span>
            <span class="n">offset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="p">],</span> <span class="n">offset</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">pad_raw</span><span class="p">:</span>
            <span class="n">raw_img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">raw_img</span><span class="p">,</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">offset</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">offset</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="p">(</span><span class="n">offset</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">offset</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="p">(</span><span class="n">offset</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">offset</span><span class="p">[</span><span class="mi">0</span><span class="p">])],</span>
                             <span class="n">mode</span><span class="o">=</span><span class="s">&#39;symmetric&#39;</span><span class="p">)</span>

        <span class="n">raw_sh</span> <span class="o">=</span> <span class="n">raw_img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>  <span class="c"># only spatial, not channels</span>

        <span class="n">tile_sh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">ps</span><span class="p">,</span> <span class="n">strides</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c"># zxy</span>
        <span class="c">#prob_sh = np.array([ps[i]+strides[i]-1-2*offset[i] for i in xrange(3)]) # zxy</span>
        <span class="n">prob_sh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">cnn_out_sh</span><span class="p">,</span> <span class="n">strides</span><span class="p">)</span>
        <span class="n">prob_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">n_lab</span><span class="p">,</span> <span class="p">],</span> <span class="n">prob_sh</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c"># zxy</span>

        <span class="n">pred_sh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">raw_sh</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">offset</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">raw_sh</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">offset</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">raw_sh</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">offset</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>  <span class="c"># xyz</span>
        <span class="k">if</span> <span class="n">as_uint8</span><span class="p">:</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="n">n_lab</span><span class="p">,</span> <span class="p">],</span> <span class="n">pred_sh</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>  <span class="c"># xyz</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="n">n_lab</span><span class="p">,</span> <span class="p">],</span> <span class="n">pred_sh</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c"># xyz</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_atleast_single_mfp</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_strides</span><span class="p">,</span> <span class="mi">1</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s">&quot;If MFP is partially enabled, the dense prediction does not work atm&quot;</span><span class="p">)</span>

        <span class="c"># Calculate number of tiles (in 3d: blocks) that need to be performed</span>
        <span class="n">x_tiles</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">pred_sh</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">prob_sh</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">y_tiles</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">pred_sh</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="n">prob_sh</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
        <span class="n">z_tiles</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">pred_sh</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="o">/</span> <span class="n">prob_sh</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">total_nb_tiles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">product</span><span class="p">([</span><span class="n">x_tiles</span><span class="p">,</span> <span class="n">y_tiles</span><span class="p">,</span> <span class="n">z_tiles</span><span class="p">])</span>
        <span class="k">print</span> <span class="s">&quot;Predicting img&quot;</span><span class="p">,</span> <span class="n">raw_img</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s">&quot;in&quot;</span><span class="p">,</span> <span class="n">total_nb_tiles</span><span class="p">,</span> <span class="s">&quot;Blocks:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">x_tiles</span><span class="p">,</span> <span class="n">y_tiles</span><span class="p">,</span> <span class="n">z_tiles</span><span class="p">)</span>
        <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">x_t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x_tiles</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">y_t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_tiles</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">z_t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">z_tiles</span><span class="p">):</span>
                    <span class="c"># For every z_tile a slice of thickness cnn_out_sh[2] is</span>
                    <span class="c"># collected and then collectively written to the output_data</span>
                    <span class="n">raw_tile</span> <span class="o">=</span> <span class="n">raw_img</span><span class="p">[:,</span> <span class="n">x_t</span> <span class="o">*</span> <span class="n">prob_sh</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span><span class="n">x_t</span> <span class="o">*</span> <span class="n">prob_sh</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">tile_sh</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                       <span class="n">y_t</span> <span class="o">*</span> <span class="n">prob_sh</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span><span class="n">y_t</span> <span class="o">*</span> <span class="n">prob_sh</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">tile_sh</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                                       <span class="n">z_t</span> <span class="o">*</span> <span class="n">prob_sh</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span><span class="n">z_t</span> <span class="o">*</span> <span class="n">prob_sh</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">tile_sh</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

                    <span class="n">this_is_end_tile</span> <span class="o">=</span> <span class="bp">False</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">raw_tile</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">np</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">tile_sh</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span> <span class="k">else</span> <span class="bp">True</span>

                    <span class="k">if</span> <span class="n">this_is_end_tile</span><span class="p">:</span>  <span class="c"># requires 0-padding</span>
                        <span class="n">right_pad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">tile_sh</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">raw_tile</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>  <span class="c"># (ch,x,y,z)</span>
                        <span class="n">right_pad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="mi">0</span><span class="p">,</span> <span class="p">],</span> <span class="n">right_pad</span><span class="p">))</span>  <span class="c"># for channel dimension</span>
                        <span class="n">left_pad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">raw_tile</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
                        <span class="n">pad_with</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">left_pad</span><span class="p">,</span> <span class="n">right_pad</span><span class="p">))</span>
                        <span class="n">raw_tile</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">raw_tile</span><span class="p">,</span> <span class="n">pad_with</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">&#39;constant&#39;</span><span class="p">)</span>

                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                        <span class="c"># slice from raw_tile(ch,x,y,z) --&gt; (ch,x,y)</span>
                        <span class="n">prob_arr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predictDenseTile</span><span class="p">(</span><span class="n">raw_tile</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">prob_arr</span><span class="p">,</span> <span class="n">offset</span><span class="p">)</span>  <span class="c"># returns (ch,z=1,x,y)</span>
                        <span class="n">prob</span> <span class="o">=</span> <span class="n">prob_arr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">]</span>  <span class="c"># (ch,z=1,x,y) -&gt; (ch,x,y,z=1)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">raw_tile</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">raw_tile</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c"># (ch,x,y,z) -&gt; (z,ch,x,y)</span>
                        <span class="n">prob_arr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predictDenseTile</span><span class="p">(</span><span class="n">raw_tile</span><span class="p">,</span> <span class="n">prob_arr</span><span class="p">,</span> <span class="n">offset</span><span class="p">)</span>
                        <span class="n">prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">prob_arr</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c"># (ch,z,x,y) -&gt; (ch,x,y,z)</span>

                    <span class="k">if</span> <span class="n">this_is_end_tile</span><span class="p">:</span>  <span class="c"># cut away padded range</span>
                        <span class="n">prob</span> <span class="o">=</span> <span class="n">prob</span><span class="p">[:,</span> <span class="p">:</span><span class="n">prob_sh</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">right_pad</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="p">:</span><span class="n">prob_sh</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">right_pad</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">:</span><span class="n">prob_sh</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">right_pad</span><span class="p">[</span><span class="mi">3</span><span class="p">]]</span>

                    <span class="k">if</span> <span class="n">as_uint8</span><span class="p">:</span>
                        <span class="n">prob</span> <span class="o">*=</span> <span class="mi">255</span>
                        <span class="n">prob</span> <span class="o">=</span> <span class="n">prob</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>  <span class="c"># maybe not needed...</span>

                    <span class="n">predictions</span><span class="p">[:,</span> <span class="n">x_t</span> <span class="o">*</span> <span class="n">prob_sh</span><span class="p">[</span><span class="mi">1</span><span class="p">]:(</span><span class="n">x_t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">prob_sh</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">y_t</span> <span class="o">*</span> <span class="n">prob_sh</span><span class="p">[</span><span class="mi">2</span><span class="p">]:(</span><span class="n">y_t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">prob_sh</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                                <span class="n">z_t</span> <span class="o">*</span> <span class="n">prob_sh</span><span class="p">[</span><span class="mi">0</span><span class="p">]:(</span><span class="n">z_t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">prob_sh</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">prob</span>

                    <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">if</span> <span class="n">show_progress</span><span class="p">:</span>
                        <span class="n">dtime</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">time_start</span>
                        <span class="n">progress</span> <span class="o">=</span> <span class="n">count</span> <span class="o">*</span> <span class="mf">100.0</span> <span class="o">/</span> <span class="n">total_nb_tiles</span>
                        <span class="n">estimate</span> <span class="o">=</span> <span class="n">dtime</span> <span class="o">/</span> <span class="n">progress</span> <span class="o">*</span> <span class="mf">100.</span>
                        <span class="k">if</span> <span class="n">progress</span> <span class="o">&lt;=</span> <span class="mi">100</span><span class="p">:</span>
                            <span class="n">dtime</span> <span class="o">=</span> <span class="n">pprinttime</span><span class="p">(</span><span class="n">dtime</span><span class="p">)</span>
                            <span class="n">estimate</span> <span class="o">=</span> <span class="n">pprinttime</span><span class="p">(</span><span class="n">estimate</span><span class="p">)</span>
                            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s">&#39;</span><span class="se">\r</span><span class="s">Progress: </span><span class="si">%.2f%%</span><span class="s"> in </span><span class="si">%s</span><span class="s">; estimate: </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">progress</span><span class="p">,</span> <span class="n">dtime</span><span class="p">,</span> <span class="n">estimate</span><span class="p">))</span>
                            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>

        <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s">&#39; - done</span><span class="se">\n</span><span class="s">&#39;</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">&quot;string_escape&quot;</span><span class="p">))</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
        <span class="k">print</span> <span class="s">&quot;Inference speed: </span><span class="si">%.3f</span><span class="s"> MB or MPix /s</span><span class="se">\n</span><span class="s">&quot;</span> <span class="o">%</span>\
              <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">*</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mi">1000000</span> <span class="o">/</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">time_start</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">strip_z</span><span class="p">:</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">predictions</span>
</div>
<div class="viewcode-block" id="MixedConvNN.get_activities"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.get_activities">[docs]</a>    <span class="k">def</span> <span class="nf">get_activities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>
        <span class="n">activities</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">dbgf</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug_functions</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)):</span>
            <span class="n">activities</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dbgf</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">activities</span>
</div>
<div class="viewcode-block" id="MixedConvNN.get_nonpooled_activities"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.get_nonpooled_activities">[docs]</a>    <span class="k">def</span> <span class="nf">get_nonpooled_activities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>
        <span class="n">activities</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">dbgf</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug_conv_output</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)):</span>
            <span class="n">activities</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dbgf</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">activities</span>
</div>
<div class="viewcode-block" id="MixedConvNN.saveParameters"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.saveParameters">[docs]</a>    <span class="k">def</span> <span class="nf">saveParameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="s">&#39;CNN.save&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Saves parameters to file, that can be loaded by ``loadParameters``&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">show</span><span class="p">:</span>
            <span class="k">print</span> <span class="s">&#39;Saving params to file&#39;</span>
        <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s">&#39;w&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">layers</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">n_lay</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_autoencoder_chains</span><span class="p">)</span>  <span class="c"># exclude the AE chains (they have only shared W)</span>
            <span class="n">layers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[:</span><span class="n">n_lay</span><span class="p">]</span>

        <span class="n">shape_info</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">lay</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
            <span class="n">shape_info</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lay</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">show</span><span class="p">:</span>
            <span class="k">print</span> <span class="s">&#39; shapes are: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">shape_info</span><span class="p">)</span>
        <span class="n">cPickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">shape_info</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">lay</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
            <span class="n">cPickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">lay</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">f</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">cPickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">lay</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">f</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lay</span><span class="o">.</span><span class="n">params</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>  <span class="c"># Recurrent Params</span>
                <span class="n">cPickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">lay</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">f</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
                <span class="n">cPickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">lay</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">f</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">cPickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">poolings</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">protocol</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c"># list of all pooling factors</span>
        <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</div>
<div class="viewcode-block" id="MixedConvNN.loadParameters"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.loadParameters">[docs]</a>    <span class="k">def</span> <span class="nf">loadParameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">myfile</span><span class="o">=</span><span class="s">&quot;CNN.save&quot;</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">n_layers_to_load</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads parameters from file created by ``saveParameters``. The parameter shapes do not need to fit the CNN</span>
<span class="sd">        architecture, they &quot;squeezed&quot; or &quot;padded&quot; to fit.</span>

<span class="sd">        Additionally the momenta of the gradients are reset</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>

<span class="sd">        myfile: string</span>
<span class="sd">          Path to file</span>
<span class="sd">        strict: bool</span>
<span class="sd">          If true, parameter shapes must fit exactly, this the only way to load RNN parameters</span>
<span class="sd">        n_layers_to_load: int</span>
<span class="sd">          Only the first x layers are initialised if this is not at its default value (-1)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">resetMomenta</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">strict</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_loadParametersStrict</span><span class="p">(</span><span class="n">myfile</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_loadParametersAdaptive</span><span class="p">(</span><span class="n">myfile</span><span class="p">,</span> <span class="n">n_layers_to_load</span><span class="o">=</span><span class="n">n_layers_to_load</span><span class="p">)</span>
</div>
    <span class="k">def</span> <span class="nf">_loadParametersAdaptive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">myfile</span><span class="o">=</span><span class="s">&quot;CNN.save&quot;</span><span class="p">,</span> <span class="n">n_layers_to_load</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a parameter set which is NOT fully compatible to the current network configuration</span>
<span class="sd">        (e.g. different filter sizes, number of filters etc).</span>

<span class="sd">        detects if layers already are in correct shape</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">print</span> <span class="s">&quot;loading(adaptive) from&quot;</span><span class="p">,</span> <span class="n">myfile</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">myfile</span><span class="p">,</span> <span class="s">&#39;r&#39;</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">print</span> <span class="s">&quot;CNN: ERROR: Cannot load file &#39;&quot;</span><span class="p">,</span> <span class="n">myfile</span><span class="p">,</span> <span class="s">&quot;&#39;&quot;</span>

        <span class="n">shp</span> <span class="o">=</span> <span class="n">cPickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">print</span> <span class="s">&quot;Shapes of loaded file are:&quot;</span><span class="p">,</span> <span class="n">shp</span>
        <span class="k">print</span> <span class="s">&quot;Shapes of current Net are:&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">lay</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">lay</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">n_layers_to_load</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">n_layers_to_load</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shp</span><span class="p">)</span>
        <span class="k">print</span> <span class="s">&quot;#Layers(sav) =&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shp</span><span class="p">),</span> <span class="s">&quot;loading&quot;</span><span class="p">,</span> <span class="n">n_layers_to_load</span>

        <span class="k">print</span> <span class="s">&quot;#Layers(CNN) =&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">it</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">it</span> <span class="o">==</span> <span class="n">n_layers_to_load</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="k">if</span> <span class="n">it</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">shp</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">p</span> <span class="o">=</span> <span class="n">cPickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="k">print</span> <span class="s">&quot;Error! &quot;</span> <span class="o">*</span> <span class="mi">7</span>
                    <span class="k">print</span> <span class="s">&quot;LoadParametersAdaptive::ERROR: invalid file, cancelled after&quot;</span><span class="p">,</span> <span class="n">it</span><span class="p">,</span> <span class="s">&quot;layers were loaded!&quot;</span>
                    <span class="n">e</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">exc_info</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="k">print</span> <span class="s">&quot;&lt;p&gt;Error: </span><span class="si">%s</span><span class="s">&lt;/p&gt;&quot;</span> <span class="o">%</span> <span class="n">e</span>
                    <span class="k">print</span> <span class="s">&quot;Error! &quot;</span> <span class="o">*</span> <span class="mi">7</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">p</span> <span class="o">=</span> <span class="p">[[[[]]]]</span>
                <span class="k">print</span> <span class="s">&quot;debug missing, might crash now!&quot;</span>
            <span class="n">save_shape</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
            <span class="n">target_shape</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>

            <span class="c">#load W</span>
            <span class="k">if</span> <span class="n">save_shape</span> <span class="o">==</span> <span class="n">target_shape</span><span class="p">:</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_shape</span><span class="p">)</span><span class="o">==</span><span class="nb">len</span><span class="p">(</span><span class="n">save_shape</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">save_shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span><span class="p">:</span>
                <span class="c">#temp param of correct shape, weights with same variance as loaded parameters (mean=0)</span>
                <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.02</span><span class="p">,</span><span class="n">target_shape</span><span class="p">))</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">&gt;</span><span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span><span class="c">#need more filters than in save</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                        <span class="k">if</span> <span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">&gt;</span><span class="n">save_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                                <span class="n">temp</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">i</span><span class="p">),</span>
                                     <span class="n">j</span><span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">j</span><span class="p">),</span> <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
                                     <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
                                    <span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="p">[:(</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="o">-</span> <span class="n">i</span><span class="p">),</span>
                                          <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">j</span><span class="p">,</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                                          <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
                                          <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])]</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">temp</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">i</span><span class="p">),</span> <span class="p">:</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                 <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
                                 <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
                                 <span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="p">[:(</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="o">-</span> <span class="n">i</span><span class="p">),</span>
                                       <span class="p">:</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                       <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
                                       <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])]</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                            <span class="n">temp</span><span class="p">[:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">j</span><span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">j</span><span class="p">),</span>
                                 <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
                                 <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
                                <span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="p">[:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                                      <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">j</span><span class="p">,</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                                      <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
                                      <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
                                     <span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                                                        <span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">j</span><span class="p">)</span><span class="o">-</span><span class="n">j</span><span class="p">,</span>
                                                        <span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
                                                        <span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
                                                        <span class="p">)</span> <span class="o">*</span> <span class="mf">1e-4</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">mid_offset</span><span class="o">=</span> <span class="mi">0</span>
                        <span class="k">if</span> <span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                            <span class="n">mid_offset</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">save_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">)</span>

                        <span class="n">temp</span><span class="p">[:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                             <span class="p">:</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                             <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="n">save_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
                             <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span><span class="n">save_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
                             <span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="p">[:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                                   <span class="n">mid_offset</span><span class="p">:</span><span class="n">mid_offset</span> <span class="o">+</span> <span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                   <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
                                   <span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])]</span>

                <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">temp</span><span class="p">,</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_shape</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">save_shape</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">save_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
                <span class="k">print</span> <span class="s">&quot;adapting 3D_net_filter...&quot;</span>
                <span class="c">#(64, 3, 32, 3, 3) #n. = 64, depth=32</span>
                <span class="c">#print &quot;fan-in correction factor =&quot;,n_params_ratio</span>

                <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-9</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">))</span>  <span class="c">#/6.*n_params_ratio</span>

                <span class="n">nf_start</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">nf_end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

                <span class="n">f_st</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">((</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                <span class="n">f_end</span> <span class="o">=</span> <span class="n">f_st</span> <span class="o">+</span> <span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

                <span class="n">f_st_</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">((</span><span class="n">save_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
                <span class="n">f_end_</span> <span class="o">=</span> <span class="n">f_st_</span> <span class="o">+</span> <span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

                <span class="n">c_st</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">c_end</span> <span class="o">=</span> <span class="n">c_st</span> <span class="o">+</span> <span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c">#</span>

                <span class="n">c_st_</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">c_end_</span> <span class="o">=</span> <span class="n">c_st_</span> <span class="o">+</span> <span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>  <span class="c">#</span>

                <span class="n">temp</span><span class="p">[</span><span class="n">nf_start</span><span class="p">:</span><span class="n">nf_end</span><span class="p">,</span> <span class="n">f_st</span><span class="p">:</span><span class="n">f_end</span><span class="p">,</span> <span class="n">c_st</span><span class="p">:</span><span class="n">c_end</span><span class="p">,</span> <span class="n">f_st</span><span class="p">:</span><span class="n">f_end</span><span class="p">,</span> <span class="n">f_st</span><span class="p">:</span><span class="n">f_end</span>
                    <span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="p">[</span><span class="n">nf_start</span><span class="p">:</span><span class="n">nf_end</span><span class="p">,</span> <span class="n">f_st_</span><span class="p">:</span><span class="n">f_end_</span><span class="p">,</span> <span class="n">c_st_</span><span class="p">:</span><span class="n">c_end_</span><span class="p">,</span> <span class="n">f_st_</span><span class="p">:</span><span class="n">f_end_</span><span class="p">,</span> <span class="n">f_st_</span><span class="p">:</span><span class="n">f_end_</span><span class="p">]</span>

                <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">temp</span><span class="p">,</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">print</span> <span class="s">&quot;Load: skipping layer&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">it</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="s">&quot;&quot;</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span> <span class="k">else</span>
                                                              <span class="s">&quot;-            can&#39;t load differently shaped perceptron layers (atm)&quot;</span><span class="p">)</span>
            <span class="c">#load b</span>
            <span class="k">if</span> <span class="n">it</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">shp</span><span class="p">):</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">cPickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">p</span> <span class="o">=</span> <span class="p">[[[[]]]]</span>
                <span class="k">print</span> <span class="s">&quot;debug missing&quot;</span>
            <span class="n">save_shape</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
            <span class="n">target_shape</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
            <span class="k">if</span> <span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">p</span><span class="p">[:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])],</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">1e-5</span> <span class="o">+</span> <span class="p">(</span><span class="mf">0.5</span> <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">activation_func</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&quot;sigmoid&quot;</span><span class="p">,</span> <span class="s">&quot;relu&quot;</span><span class="p">]</span> <span class="k">else</span> <span class="mi">0</span><span class="p">),</span>
                                                    <span class="mf">1e-6</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                    <span class="n">temp</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">p</span><span class="p">[:</span><span class="nb">min</span><span class="p">(</span><span class="n">target_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">i</span><span class="p">,</span> <span class="n">save_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">temp</span><span class="p">,</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">print</span> <span class="s">&quot;loading complete&quot;</span>

        <span class="c">#function lacks error-handling</span>
    <span class="k">def</span> <span class="nf">_loadParametersStrict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">myfile</span><span class="o">=</span><span class="s">&quot;CNN.save&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a parameter set which is **fully compatible** to</span>
<span class="sd">        the current network configuration (FAILS otherwise).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">print</span> <span class="s">&quot;Loading from&quot;</span><span class="p">,</span> <span class="n">myfile</span>
        <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">myfile</span><span class="p">,</span> <span class="s">&#39;r&#39;</span><span class="p">)</span>

        <span class="n">shp</span> <span class="o">=</span> <span class="n">cPickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">print</span> <span class="s">&quot;Shapes are:&quot;</span><span class="p">,</span> <span class="n">shp</span>
        <span class="k">print</span> <span class="s">&quot;#Layers =&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shp</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">cPickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">cPickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
            <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">cPickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">cPickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
                <span class="n">layer</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">borrow</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<div class="viewcode-block" id="MixedConvNN.gradstats"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.gradstats">[docs]</a>    <span class="k">def</span> <span class="nf">gradstats</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug_gradients_function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&quot;Gradient statistics&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">&quot;shape=</span><span class="si">%s</span><span class="s">,</span><span class="se">\t</span><span class="s">mean=</span><span class="si">%f</span><span class="s">,</span><span class="se">\t</span><span class="s">std=</span><span class="si">%f</span><span class="s">&quot;</span> <span class="o">%</span>
                  <span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">g</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">g</span><span class="p">)))</span>
</div>
<div class="viewcode-block" id="MixedConvNN.actstats"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.actstats">[docs]</a>    <span class="k">def</span> <span class="nf">actstats</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">acts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_activities</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&quot;Activation statistics&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">acts</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">&quot;shape=</span><span class="si">%s</span><span class="s">,</span><span class="se">\t</span><span class="s">mean=</span><span class="si">%f</span><span class="s">,</span><span class="se">\t</span><span class="s">std=</span><span class="si">%f</span><span class="s">&quot;</span> <span class="o">%</span>
                  <span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">a</span><span class="p">)))</span>
</div>
<div class="viewcode-block" id="MixedConvNN.paramstats"><a class="viewcode-back" href="../../../elektronn.net.html#elektronn.net.convnet.MixedConvNN.paramstats">[docs]</a>    <span class="k">def</span> <span class="nf">paramstats</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&quot;Parameters statistics&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">get_value</span><span class="p">()</span>
            <span class="k">print</span><span class="p">(</span><span class="s">&quot;shape=</span><span class="si">%s</span><span class="s">,</span><span class="se">\t</span><span class="s">mean=</span><span class="si">%f</span><span class="s">,</span><span class="se">\t</span><span class="s">std=</span><span class="si">%f</span><span class="s">&quot;</span> <span class="o">%</span>
                  <span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">p</span><span class="p">)))</span></div></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">ELEKTRONN</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="../../index.html" >Module code</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright 2015, Gregor Urban, Marius F Killinger.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.1.
    </div>
  </body>
</html>