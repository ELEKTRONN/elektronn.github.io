<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Examples &mdash; ELEKTRONN</title>
    
    <link rel="stylesheet" href="_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1rc',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="shortcut icon" href="_static/elektronnfavicon.ico"/>
    <link rel="top" title="ELEKTRONN" href="index.html" />
    <link rel="next" title="Practical Introduction to Neural Networks" href="IntroNN.html" />
    <link rel="prev" title="Installation" href="Installation.html" /> 
  </head>
  <body role="document">

<div style="background-color: white; text-align: left; padding: 10px 10px 15px 15px">
<a href="index.html"><img src="_static/elektronn.png" border="0" alt="sampledoc"/></a>
</div>

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="IntroNN.html" title="Practical Introduction to Neural Networks"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="Installation.html" title="Installation"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">ELEKTRONN</a> &raquo;</li> 
      </ul>
    </div>

      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Examples</a><ul>
<li><a class="reference internal" href="#d-neuro-data">3D Neuro Data</a><ul>
<li><a class="reference internal" href="#getting-started">Getting Started</a></li>
<li><a class="reference internal" href="#data-set">Data Set</a></li>
<li><a class="reference internal" href="#cnn-design">CNN design</a></li>
<li><a class="reference internal" href="#training-data-options">Training Data Options</a><ul>
<li><a class="reference internal" href="#results-comments">Results &amp; Comments</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#mnist-examples">MNIST Examples</a><ul>
<li><a class="reference internal" href="#cnn-with-built-in-pipeline">CNN with built-in Pipeline</a><ul>
<li><a class="reference internal" href="#id3">Results &amp; Comments</a></li>
</ul>
</li>
<li><a class="reference internal" href="#mlp-with-built-in-pipeline">MLP with built-in Pipeline</a></li>
<li><a class="reference internal" href="#standalone-cnn">Standalone CNN</a></li>
<li><a class="reference internal" href="#auto-encoder-example">Auto encoder Example</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="Installation.html"
                        title="previous chapter">Installation</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="IntroNN.html"
                        title="next chapter">Practical Introduction to Neural Networks</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/Examples.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="examples">
<span id="id1"></span><h1>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h1>
<p>This page gives examples for different use cases of ELELKTRONN. Besides, the examples are intended to give an idea of how custom network architectures could be created and trained without the built-in pipeline. To understand the examples, basic knowledge of neural networks (e.g. from <a class="reference internal" href="IntroNN.html#training"><span>Practical Introduction to Neural Networks</span></a>) is recommended. The details of the configuration parameters are described <a class="reference internal" href="Pipeline.html#pipeline"><span>here</span></a>.</p>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#d-neuro-data" id="id9">3D Neuro Data</a><ul>
<li><a class="reference internal" href="#getting-started" id="id10">Getting Started</a></li>
<li><a class="reference internal" href="#data-set" id="id11">Data Set</a></li>
<li><a class="reference internal" href="#cnn-design" id="id12">CNN design</a></li>
<li><a class="reference internal" href="#training-data-options" id="id13">Training Data Options</a></li>
</ul>
</li>
<li><a class="reference internal" href="#mnist-examples" id="id14">MNIST Examples</a><ul>
<li><a class="reference internal" href="#cnn-with-built-in-pipeline" id="id15">CNN with built-in Pipeline</a></li>
<li><a class="reference internal" href="#mlp-with-built-in-pipeline" id="id16">MLP with built-in Pipeline</a></li>
<li><a class="reference internal" href="#standalone-cnn" id="id17">Standalone CNN</a></li>
<li><a class="reference internal" href="#auto-encoder-example" id="id18">Auto encoder Example</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="d-neuro-data">
<h2><a class="toc-backref" href="#id9">3D Neuro Data</a><a class="headerlink" href="#d-neuro-data" title="Permalink to this headline">¶</a></h2>
<p>This task is about detecting neuron cell boundaries in 3D electron microscopy image volumes. The more general goal is to find a segmentation by assigning each voxel a cell ID. Predicting boundaries is surrogate target for which a CNN can be trained (see also the note about target formulation <a class="reference internal" href="Pipeline.html#data-format"><span>here</span></a>) - the actual segmentation would be made by e.g. running a watershed on the predicted boundary map. This is a typical <em>img-img</em> task.</p>
<p>For demonstration purpose a very small CNN with only 70k parameters and 5 layers is used. This trains fast but is obviously limited in accuracy. Besides, to solve this task well, more training data would be required.</p>
<p>The full configuration file can be found in ELEKTRONN&#8217;s <code class="docutils literal"><span class="pre">examples</span></code> folder as  <code class="docutils literal"><span class="pre">neuro_3d_config.py</span></code>. Here only selected settings will be mentioned.</p>
<div class="section" id="getting-started">
<h3><a class="toc-backref" href="#id10">Getting Started</a><a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h3>
<ol class="arabic">
<li><p class="first">Download <a class="reference external" href="http://elektronn.org/downloads/neuro_data.zip">example training data</a>  (~100MB):</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>wget http://elektronn.org/downloads/neuro_data.zip
unzip neuro_data.zip
</pre></div>
</div>
</li>
<li><p class="first">In the config file <code class="docutils literal"><span class="pre">neuro_3d_config.py</span></code> edit <code class="docutils literal"><span class="pre">save_path,</span> <span class="pre">data_path,</span> <span class="pre">label_path,</span> <span class="pre">preview_data_path</span></code></p>
</li>
<li><p class="first">Run:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>elektronn-train &lt;/path/to_config_file&gt; [ --gpu={Auto|False|&lt;int&gt;}]
</pre></div>
</div>
</li>
<li><p class="first">Inspect the printed output and the plots in the save directory</p>
</li>
</ol>
</div>
<div class="section" id="data-set">
<h3><a class="toc-backref" href="#id11">Data Set</a><a class="headerlink" href="#data-set" title="Permalink to this headline">¶</a></h3>
<p>There are 3 volumes that contain &#8220;barrier&#8221; labels (union of cell boundaries and extra cellular space) of shape <code class="docutils literal"><span class="pre">(150,150,150)</span></code> in <code class="docutils literal"><span class="pre">(x,y,z)</span></code> axis order . Correspondingly there are 3 volumes that contain the raw electron microscopy images. Because a CNN can only make predictions within some offset from the input image extent, the size of the image cubes is larger <code class="docutils literal"><span class="pre">(350,350,250)</span></code> in order to be able to make predictions (and to train!) for every labelled voxel. The margin in this examples allows to make predictions for the labelled region with a maximal field of view of <code class="docutils literal"><span class="pre">201</span></code> in  <code class="docutils literal"><span class="pre">x,y</span></code> and <code class="docutils literal"><span class="pre">101</span></code> in <code class="docutils literal"><span class="pre">z</span></code>.</p>
<p>There is a difference in the lateral dimensions and in <code class="docutils literal"><span class="pre">z</span></code> - direction because this data set is anisotropic: lateral voxels have a spacing of <img class="math" src="_images/math/a61b546602c6a67adddca70ab5042bfd4820f7e3.png" alt="10 \mu m"/> in contrast to <img class="math" src="_images/math/3b86097c6ed1d01c713f7a98e7877647afe857c8.png" alt="20 \mu m"/> vertically. Snapshots of images and labels are depicted below.</p>
<p>During training the pipeline cuts image and target patches from the loaded data cubes at randomly sampled locations and feeds them to the CNN. Therefore the CNN input size should be smaller than the size of the cubes, to leave enough space to cut from many different positions. Otherwise it will always use the same patch (more or less) and soon over-fit to that one.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Implementation details:</strong> When the cubes are read into the pipeline it is implicitly assumed that the smaller label cube is spatially centered w.r.t the larger image cube (hence the size surplus of the image cube must be even). Furthermore the cubes are for performance reasons internally axis swapped to <code class="docutils literal"><span class="pre">(z,</span> <span class="pre">(ch,)</span> <span class="pre">x,</span> <span class="pre">y)</span></code> order, zero-padded to the same size and cropped such that only the area in which labels and images are both available after considering the CNN offset. If labels cannot be effectively used for training (because either the image surplus is to small or your FOV is to larger) a note will be printed.</p>
</div>
<p>Additionally to the 3 pairs of images and labels, 2 image cubes for live previews are included. Note that preview data must be a <strong>list</strong> of one or several cubes in a <code class="docutils literal"><span class="pre">h5</span></code>-file.</p>
</div>
<div class="section" id="cnn-design">
<h3><a class="toc-backref" href="#id12">CNN design</a><a class="headerlink" href="#cnn-design" title="Permalink to this headline">¶</a></h3>
<p>The architecture of the CNN is determined by:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">n_dim</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">filters</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">pool</span>    <span class="o">=</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">nof_filters</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">40</span><span class="p">]</span>
<span class="n">desired_input</span> <span class="o">=</span> <span class="p">[</span><span class="mi">127</span><span class="p">,</span><span class="mi">127</span><span class="p">,</span><span class="mi">7</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
<ul class="simple">
<li>Because the data is anisotropic the lateral FOV is chosen to be larger. This reduces the computational complexity compared to a naive isotropic CNN. Even for genuinely isotropic data this might be a useful strategy, if it is plausible that seeing a large lateral context is sufficient to solve the task.</li>
<li>As an extreme, the presented CNN is partially actually 2D: in the first two and in the last layer the filter kernels have extent <code class="docutils literal"><span class="pre">1</span></code> in <code class="docutils literal"><span class="pre">z</span></code>. Only two middle layers perform a truly 3D aggregation of the features along the third axis.</li>
<li>The resulting FOV is <code class="docutils literal"><span class="pre">[31,31,7]</span></code> (to solve this task well, more than <code class="docutils literal"><span class="pre">100</span></code> lateral FOV is beneficial...)</li>
<li>Using this input size gives an output shape of <code class="docutils literal"><span class="pre">[25,25,3]</span></code> i.e. 1875 prediction neurons. For training, this is a good compromise between computational cost and sufficiently many prediction neurons to average the gradient over. Too few output pixel result in so noisy gradients that convergence might be impossible. For making predictions, it is more efficient to re-created the CNN with a larger input size (see <a class="reference internal" href="Prediction.html#mfp"><span>here</span></a>).</li>
<li>If there are several <code class="docutils literal"><span class="pre">100-1000</span></code> output neurons, a batch size of <code class="docutils literal"><span class="pre">1</span></code> is commonly sufficient and is not necessary to compute an average gradient over several images.</li>
<li>The output shape has strides of <code class="docutils literal"><span class="pre">[4,4,1]</span></code> due to 2 times lateral pooling by 2. This means that the predicted <code class="docutils literal"><span class="pre">[25,25,3]</span></code> voxels do not lie laterally adjacent, if projected back to the space of the input image: for every lateral output voxel there are <code class="docutils literal"><span class="pre">3</span></code> voxel separating it from the next output voxel - for those no prediction is available. To obtain dense predictions (e.g. when making the live previews) the method <a class="reference internal" href="elektronn.net.html#elektronn.net.convnet.MixedConvNN.predictDense" title="elektronn.net.convnet.MixedConvNN.predictDense"><code class="xref py py-meth docutils literal"><span class="pre">elektronn.net.convnet.MixedConvNN.predictDense()</span></code></a> is used, which moves along the missing locations and stitches the results. For making large scale predictions after training, this can be done more efficiently using MFP (see <a class="reference internal" href="Prediction.html#mfp"><span>here</span></a>).</li>
<li>To solve this task well, about twice the number of layers, several million parameters and more training data are needed.</li>
</ul>
</div>
<div class="section" id="training-data-options">
<h3><a class="toc-backref" href="#id13">Training Data Options</a><a class="headerlink" href="#training-data-options" title="Permalink to this headline">¶</a></h3>
<p>Config:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">valid_cubes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,]</span>
<span class="n">grey_augment_channels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">flip_data</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">anisotropic_data</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">warp_on</span> <span class="o">=</span> <span class="mf">0.7</span>
</pre></div>
</div>
<ul>
<li><p class="first">Of the three training data cubes the last one is used as validation data.</p>
</li>
<li><p class="first">The input images are grey-valued i.e. they have only 1 channel. For this channel &#8220;grey value augmentaion&#8221; (randomised histogram distortions) are applied when sampling batches during training. This helps to achieve invariance against varying contrast and brightness gradients.</p>
</li>
<li><p class="first">During patch cutting the axes are flipped and transposed as a means of data augmentation</p>
</li>
<li><p class="first">If the data is anisotropic, the pipeline assumes that the singled-out axis is <code class="docutils literal"><span class="pre">z</span></code>. For anisotropic data axes are not transposed in a way that axes of different resolution get mixed up.</p>
</li>
<li><p class="first">For 70% of the batches the image and labels are randomly <a class="reference internal" href="IntroNN.html#warping"><span>warped</span></a></p>
<div class="figure align-center">
<img alt="_images/debugGetCNNBatch.png" src="_images/debugGetCNNBatch.png" />
</div>
<p>Left: the input data. Centre: the labels, note the offset, Right: overlay of data with labels, here you can check whether they are properly registered.</p>
</li>
</ul>
<p>During training initialisation a debug plot of a randomly sampled batch is made to check whether the training data is presented to the CNN in the intended way and to find errors (e.g. image and label cubes are not matching or labels are shifted w.r.t to images). Once the training loop has started, more such plots can be made from the ELEKTRONN command line (<code class="docutils literal"><span class="pre">ctrl+c</span></code>)</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">mfk</span><span class="nd">@ELEKTRONN</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">debugGetCNNBatch</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Training with 2D images</strong>:
The shown setup works likewise for training a 2D CNN on this task. Just the CNN configuration parameters must be adjusted.
Then 2D training patches are cut from the cubes. If <code class="docutils literal"><span class="pre">anisotropic_data</span> <span class="pre">=</span> <span class="pre">True</span></code> these are cut only from the <code class="docutils literal"><span class="pre">x,y</span></code>-plane; otherwise transposed, too.
Therefore, this setup can be used for actual 2D images if they are stacked to form a cube along a new &#8220;<code class="docutils literal"><span class="pre">z</span></code>&#8220;-axis. If the 2D images have different shapes they cannot be stacked but, the 2D arrays can be augmented with a third dummy-axis to be of shape <code class="docutils literal"><span class="pre">(x,y,1)</span></code> and each put in a separate <code class="docutils literal"><span class="pre">h5</span></code>-file, which is slightly more intricate.</p>
</div>
<div class="section" id="results-comments">
<h4>Results &amp; Comments<a class="headerlink" href="#results-comments" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>When running this example, commonly the NLL-loss stagnates for about <code class="docutils literal"><span class="pre">15k</span></code> iterations around <code class="docutils literal"><span class="pre">0.7</span></code>. After that you should observe a clear decrease. On a desktop with a high-end GPU, with latest theano and cuDNN versions and using background processes for the batch creation the training should run <code class="docutils literal"><span class="pre">at</span> <span class="pre">15-10</span> <span class="pre">it/s</span></code>.</li>
<li>Because of the (too) small training data size the validation error should stagnate soon and even go up later.</li>
<li>Because the model has too few parameters, predictions are typically not smooth and exhibit grating-like patterns - using a more complex model mitigates this effect.</li>
<li>Because the model has a small FOV (which for this task should rather be increase by more layers than more maxpooling) predictions contain a lot of &#8220;clutter&#8221; within wide cell bodies: there the CNN does not see the the cell outline which is apparently an important clue to solve this task.</li>
</ul>
<div class="figure align-center" id="id4">
<img alt="_images/barrier_training_example.gif" src="_images/barrier_training_example.gif" />
<p class="caption"><span class="caption-text">Preview predictions of this example model trained over 2h.</span></p>
</div>
<div class="figure align-center" id="id5">
<img alt="_images/barrier_training.gif" src="_images/barrier_training.gif" />
<p class="caption"><span class="caption-text">Preview predictions of a more complex model composed of 9 convolutional layers, <code class="docutils literal"><span class="pre">1.5M</span></code> parameters and <code class="docutils literal"><span class="pre">83</span></code> lateral FOV, trained on 9 cubes for 16h.</span></p>
</div>
</div>
</div>
</div>
<div class="section" id="mnist-examples">
<span id="mnist"></span><h2><a class="toc-backref" href="#id14">MNIST Examples</a><a class="headerlink" href="#mnist-examples" title="Permalink to this headline">¶</a></h2>
<p>MNIST is a benchmark data set for handwritten digit recognition/classification. State of the art benchmarks for comparison can be found <a class="reference external" href="http://yann.lecun.com/exdb/mnist/">here</a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The data will be automatically downloaded but can also be downloaded manually from <a class="reference external" href="http://www.elektronn.org/downloads/mnist.pkl.gz">here</a>.</p>
</div>
<div class="section" id="cnn-with-built-in-pipeline">
<h3><a class="toc-backref" href="#id15">CNN with built-in Pipeline</a><a class="headerlink" href="#cnn-with-built-in-pipeline" title="Permalink to this headline">¶</a></h3>
<p>In ELEKTRONN&#8217;s <code class="docutils literal"><span class="pre">examples</span></code> folder is a file <code class="docutils literal"><span class="pre">MNIST_CNN_warp_config.py</span></code>. This is a configuration for <em>img-scalar</em> training and it uses a different data class than the &#8220;big&#8221; pipeline for neuro data. When using an alternative data pipeline, the options for data loading and batch creation are given given by keyword argument dictionaries in the <code class="docutils literal"><span class="pre">Data</span> <span class="pre">Alternative</span></code> section of the config file:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">data_class_name</span>      <span class="o">=</span> <span class="s1">&#39;MNISTData&#39;</span>
<span class="n">data_load_kwargs</span>     <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">convert2image</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">warp_on</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">shift_augment</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">data_batch_kwargs</span>    <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</pre></div>
</div>
<p>This configuration results in:</p>
<blockquote>
<div><ul class="simple">
<li>Initialising a data class adapted for MNIST from <a class="reference internal" href="elektronn.training.html#module-elektronn.training.traindata" title="elektronn.training.traindata"><code class="xref py py-mod docutils literal"><span class="pre">elektronn.training.traindata</span></code></a></li>
<li>Downloading the MNIST data automatically if path is <code class="docutils literal"><span class="pre">None</span></code> (otherwise the given path is used)</li>
<li>Reshaping the &#8220;flat&#8221; training examples (they are stored as vectors of length 784) to <code class="docutils literal"><span class="pre">28</span> <span class="pre">x</span> <span class="pre">28</span></code> matrices i.e. images</li>
<li>Data augmentation through warping (see <a class="reference internal" href="IntroNN.html#warping"><span>Data Augmentation</span></a>): for each batch in a training iteration random deformation parameters are sampled and the corresponding transformations are applied to the images in a background process.</li>
<li>Data augmentation through translation: <code class="docutils literal"><span class="pre">shift_augment</span></code> crops the <code class="docutils literal"><span class="pre">28</span> <span class="pre">x</span> <span class="pre">28</span></code> images  to <code class="docutils literal"><span class="pre">26</span> <span class="pre">x</span> <span class="pre">26</span></code> (you may notice this in the printed output). The cropping leaves choice of the origin (like applying small translations), in this example the data set size is inflated by factor <code class="docutils literal"><span class="pre">4</span></code>.</li>
<li>For the function <code class="docutils literal"><span class="pre">getbatch</span></code> no additional kwargs are required (the warping and so on was specified already with the initialisation).</li>
</ul>
</div></blockquote>
<p>The architecture of the NN is determined by:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">n_dim</span>           <span class="o">=</span> <span class="mi">2</span>                     <span class="c1"># MNIST are 2D images</span>
<span class="n">desired_input</span>   <span class="o">=</span> <span class="mi">26</span>
<span class="n">filters</span>         <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>                 <span class="c1"># two conv layers with each 3x3 filters</span>
<span class="n">pool</span>            <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span>                 <span class="c1"># for each conv layer maxpooling by 2x2</span>
<span class="n">nof_filters</span>     <span class="o">=</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="mi">32</span><span class="p">]</span>               <span class="c1"># number of feature maps per layer</span>
<span class="n">MLP_layers</span>       <span class="o">=</span> <span class="p">[</span><span class="mi">300</span><span class="p">,</span><span class="mi">300</span><span class="p">]</span>            <span class="c1"># numbers of filters for perceptron layers (after conv layers)</span>
</pre></div>
</div>
<p>This is 2D CNN with two conv layers and two fully connected layers each with 300 neurons. As MNIST has 10 classes, an output layer with 10 neurons is automatically added, and not specified here.</p>
<p>To run the example, make a copy of the config file and adjust the paths. Then run the <code class="docutils literal"><span class="pre">elektronn-train</span></code> script, and pass the path of your config file:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>elektronn-train &lt;/path/to_config_file&gt; [ --gpu={Auto|False|&lt;int&gt;}]
</pre></div>
</div>
<p>The output should read like this:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span>Reading config-file ../elektronn/examples/MNIST_CNN_warp_config.py
WARNING: Receptive Fields are not centered with even field of view (10)
WARNING: Receptive Fields are not centered with even field of view (10)
Selected patch-size for CNN input: Input: [26, 26]
Layer/Fragment sizes: [[12, 5], [12, 5]]
Unpooled Layer sizes: [[24, 10], [24, 10]]
Receptive fields:     [[4, 10], [4, 10]]
Strides:              [[2, 4], [2, 4]]
Overlap:              [[2, 6], [2, 6]]
Offset:               [5.0, 5.0].
If offset is non-int: output neurons lie centered on input neurons,they have an odd FOV

Overwriting existing save directory: /home/mfk/CNN_Training/2D/MNIST_example_warp/
Using gpu device 0: GeForce GTX TITAN
Load ELEKTRONN Core
10-class Data Set: #training examples: 200000 and #validing: 10000
MNIST data is converted/augmented to shape (1, 26, 26)
------------------------------------------------------------
Input shape   =  (50, 1, 26, 26) ; This is a 2 dimensional NN
---
2DConv: input= (50, 1, 26, 26)        filter= (16, 1, 3, 3)
Output = (50, 16, 12, 12) Dropout OFF, Act: relu pool: max
Computational Cost: 4.1 Mega Ops
---
2DConv: input= (50, 16, 12, 12)       filter= (32, 16, 3, 3)
Output = (50, 32, 5, 5) Dropout OFF, Act: relu pool: max
Computational Cost: 23.0 Mega Ops
---
PerceptronLayer( #Inputs = 800 #Outputs = 300 )
Computational Cost: 12.0 Mega Ops
---
PerceptronLayer( #Inputs = 300 #Outputs = 300 )
Computational Cost: 4.5 Mega Ops
---
PerceptronLayer( #Inputs = 300 #Outputs = 10 )
Computational Cost: 150.0 kilo Ops
---
GLOBAL
Computational Cost: 43.8 Mega Ops
Total Count of trainable Parameters: 338410
Building Computational Graph took 0.030 s
Compiling output functions for nll target:
        using no class_weights
        using no example_weights
        using no lazy_labels
        label propagation inactive
</pre></div>
</div>
<p>A few comments on the expected output before training:</p>
<blockquote>
<div><ul class="simple">
<li>There will be a warning that receptive fields are not centered (the neurons in the last conv layer lie spatially &#8220;between&#8221; the neurons of the input layer). This is ok because this training task does require localisation of objects. All local information is discarded anyway when the fully connected layers are put after the conv layers.</li>
<li>The information of <a class="reference internal" href="elektronn.net.html#elektronn.net.netutils.CNNCalculator" title="elektronn.net.netutils.CNNCalculator"><code class="xref py py-func docutils literal"><span class="pre">elektronn.net.netutils.CNNCalculator()</span></code></a> is printed first, i.e. the layer sizes, receptive fields etc.</li>
<li>Although MNIST contains only 50000 training examples, it will print 200000 because of the shift augmentation, which is done when loading the data</li>
<li>For image training, an auxiliary dimension for the (colour) channel is introduced.</li>
<li>The input shape <code class="docutils literal"><span class="pre">(50,</span> <span class="pre">1,</span> <span class="pre">26,</span> <span class="pre">26)</span></code> indicates that the batch size is 50, the number of channels is just 1 and the image extent is <code class="docutils literal"><span class="pre">26</span> <span class="pre">x</span> <span class="pre">26</span></code>.</li>
<li>You can observe that the first layer outputs an image of size is <code class="docutils literal"><span class="pre">12</span> <span class="pre">x</span> <span class="pre">12</span></code>: the convolution with filter size 3 reduces 26 to 24, then the maxpooling by factor 2 reduces 24 to 12.</li>
<li>After the last conv layer everything except the batch dimension is flattened to be feed into a fully connected layer: <code class="docutils literal"><span class="pre">32</span> <span class="pre">x</span> <span class="pre">5</span> <span class="pre">x</span> <span class="pre">5</span> <span class="pre">==</span> <span class="pre">800</span></code>. If the image extent is not sufficiently small before doing this (e.g. <code class="docutils literal"><span class="pre">10</span> <span class="pre">x</span> <span class="pre">10</span> <span class="pre">==</span> <span class="pre">100</span></code>) this will be a bottleneck and introduce <strong>huge</strong> weight matrices for the fully connected layer; more poolings must be used then.</li>
</ul>
</div></blockquote>
<div class="section" id="id3">
<h4>Results &amp; Comments<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<p>The values in the example file should give a good result after about 10-15 minutes on a recent GPU, but you are invited to play around with the network architecture and meta-parameters such as the learning rate. To watch the progress (in a nicer way than the reading the printed numbers on the console) go to the save directory and have a look at the plots. Every time a new line is printed in the console, the plot gets updated as well.</p>
<p><strong>If you had not used warping</strong> the progress of the training would look like this:</p>
<div class="figure align-center" id="id6">
<img alt="_images/MNIST_Nowarp.Errors.png" src="_images/MNIST_Nowarp.Errors.png" />
<p class="caption"><span class="caption-text">Withing a few minutes the <em>training</em> error goes to 0 whereas the <em>validation</em> error  stays on a higher level.</span></p>
</div>
<p>The spread between training and validation set (a partition of the data not presented as training examples) indicates a kind of over-fitting. But actually the over-fitting observed here is not as bad as it could be: because the training error is 0 the gradients are close to 0 - no weight updates are made for 0 gradient, so the training stops &#8220;automatically&#8221; at this point. For different data sets the training error might not reach 0 and weight updates are made all the time resulting in a validation error that goes <strong>up</strong> after some time - this would be real over-fitting.</p>
<p>A common regularisation technique to prevent over-fitting is drop out which is also implemented in ELEKETRONN. But since MNIST data are images, we want to demonstrate the use of warping instead in this example.</p>
<p>Warping makes the training goal more difficult, therefore the CNN has to learn its task &#8220;more thoroughly&#8221;. This greatly reduces the spread between training and validation set. Training also takes slightly more time. And because the task is more difficult the training error will not reach 0 anymore. The validation error is also high during training, since the CNN is devoting resources to solving the difficult (warped) training set at the expense of generalization to &#8220;normal&#8221; data of the validation set.</p>
<p>The actual boost in (validation) performance comes when the warping is turned off and the training is fine-tuned with a smaller learning rate. Wait until the validation error approximately plateaus, then interrupt the training using <code class="docutils literal"><span class="pre">ctrl+c</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span><span class="o">.</span><span class="n">warp_on</span> <span class="o">=</span> <span class="bp">False</span> <span class="c1"># Turn off warping</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">setlr</span> <span class="mf">0.002</span>          <span class="c1"># Lower learning rate</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span>                    <span class="c1"># quit console to continue training</span>
</pre></div>
</div>
<p>This stops the warping for further training and lowers the learning rate.
The resulting training progress would look like this:</p>
<div class="figure align-center" id="id7">
<img alt="_images/MNIST_warp.Errors.png" src="_images/MNIST_warp.Errors.png" />
<p class="caption"><span class="caption-text">The training was interrupted after ca. 130000 iterations. Turning off warping reduced both errors to their final level (after the gradient is 0 again, no progress can be made).</span></p>
</div>
<p>Because our decisions on the best learning rate and the best point to stop warping have been influenced by the validation set (we could somehow over-fit to the validation set), the actual performance is evaluated on a separate, third set, the <em>test</em> set (we should really only ever look at the test error when we have decided on a training setup/schedule, the test set is not meant to influence training at all).</p>
<p>Stop the training using <code class="docutils literal"><span class="pre">ctrl+c</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="bp">self</span><span class="o">.</span><span class="n">testModel</span><span class="p">(</span><span class="s1">&#39;test&#39;</span><span class="p">)</span>
<span class="go">(&lt;NLL&gt;, &lt;Errors&gt;)</span>
</pre></div>
</div>
<p>The result should be competitive - around 0.5% error, i.e. 99.5% accuracy.</p>
</div>
</div>
<div class="section" id="mlp-with-built-in-pipeline">
<h3><a class="toc-backref" href="#id16">MLP with built-in Pipeline</a><a class="headerlink" href="#mlp-with-built-in-pipeline" title="Permalink to this headline">¶</a></h3>
<p>In the spirit of the above example, MNIST can also be trained with a pure multi layer perceptron (MLP) without convolutions. The images are then just flattened vectors (&#8211;&gt; <em>vect-scalar</em> mode). There is a config file <code class="docutils literal"><span class="pre">MNIST_MLP_config.py</span></code> in the <code class="docutils literal"><span class="pre">Examples</span></code> folder. This method can also be applied for any other non-image data, e.g. predicting income from demographic features.</p>
</div>
<div class="section" id="standalone-cnn">
<h3><a class="toc-backref" href="#id17">Standalone CNN</a><a class="headerlink" href="#standalone-cnn" title="Permalink to this headline">¶</a></h3>
<p>If you think the big pipeline and long configuration file is a bit of an overkill for good old MNIST we have an alternative lightweight example in the file <code class="docutils literal"><span class="pre">MNIST_CNN_standalone.py</span></code> of the <code class="docutils literal"><span class="pre">Examples</span></code> folder. This example illustrates what (in a slightly more elaborate way) happens under the hood of the big pipeline.</p>
<p>First we import the required classes and initialise a training data object from <a class="reference internal" href="elektronn.training.html#module-elektronn.training.traindata" title="elektronn.training.traindata"><code class="xref py py-mod docutils literal"><span class="pre">elektronn.training.traindata</span></code></a> (which we actually used above, too). It does not more than loading the training, validation and testing data and sample batches randomly - all further options e.g. for augmentation are not used here:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">elektronn.training.traindata</span> <span class="kn">import</span> <span class="n">MNISTData</span>
<span class="kn">from</span> <span class="nn">elektronn.net.convnet</span> <span class="kn">import</span> <span class="n">MixedConvNN</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">MNISTData</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s1">&#39;~/devel/ELEKTRONN/Examples/mnist.pkl&#39;</span><span class="p">,</span><span class="n">convert2image</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">shift_augment</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Next we set up the Neural Network. Each method of <code class="docutils literal"><span class="pre">cnn</span></code> has much more options which are explained in the API doc. Start with similar code if you want to create customised NNs:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">cnn</span> <span class="o">=</span> <span class="n">MixedConvNN</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">),</span><span class="n">input_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># input_depth: only 1 gray channel (no RGB or depth)</span>
<span class="n">cnn</span><span class="o">.</span><span class="n">addConvLayer</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span> <span class="n">pool_shape</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation_func</span><span class="o">=</span><span class="s2">&quot;abs&quot;</span><span class="p">)</span> <span class="c1"># (nof, filtersize)</span>
<span class="n">cnn</span><span class="o">.</span><span class="n">addConvLayer</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pool_shape</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation_func</span><span class="o">=</span><span class="s2">&quot;abs&quot;</span><span class="p">)</span>
<span class="n">cnn</span><span class="o">.</span><span class="n">addPerceptronLayer</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation_func</span><span class="o">=</span><span class="s2">&quot;abs&quot;</span><span class="p">)</span>
<span class="n">cnn</span><span class="o">.</span><span class="n">addPerceptronLayer</span><span class="p">(</span><span class="mi">80</span><span class="p">,</span> <span class="n">activation_func</span><span class="o">=</span><span class="s2">&quot;abs&quot;</span><span class="p">)</span>
<span class="n">cnn</span><span class="o">.</span><span class="n">addPerceptronLayer</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation_func</span><span class="o">=</span><span class="s2">&quot;abs&quot;</span><span class="p">)</span> <span class="c1"># need 10 outputs as there are 10 classes in the data set</span>
<span class="n">cnn</span><span class="o">.</span><span class="n">compileOutputFunctions</span><span class="p">()</span>
<span class="n">cnn</span><span class="o">.</span><span class="n">setOptimizerParams</span><span class="p">(</span><span class="n">SGD</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;LR&#39;</span><span class="p">:</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">},</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># LR: learning rate</span>
</pre></div>
</div>
<p>Finally, the training loop which applies weight updates in every iteration:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5000</span><span class="p">):</span>
  <span class="n">d</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">getbatch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
  <span class="n">loss</span><span class="p">,</span> <span class="n">loss_instance</span><span class="p">,</span> <span class="n">time_per_step</span> <span class="o">=</span> <span class="n">cnn</span><span class="o">.</span><span class="n">trainingStep</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;SGD&quot;</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="mi">100</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">valid_loss</span><span class="p">,</span> <span class="n">valid_error</span><span class="p">,</span> <span class="n">valid_predictions</span> <span class="o">=</span> <span class="n">cnn</span><span class="o">.</span><span class="n">get_error</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">valid_d</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">valid_l</span><span class="p">)</span>
        <span class="k">print</span> <span class="s2">&quot;update:&quot;</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="s2">&quot;; Validation loss:&quot;</span><span class="p">,</span><span class="n">valid_loss</span><span class="p">,</span> <span class="s2">&quot;Validation error:&quot;</span><span class="p">,</span><span class="n">valid_error</span><span class="o">*</span><span class="mf">100.</span><span class="p">,</span><span class="s2">&quot;%&quot;</span>

<span class="n">loss</span><span class="p">,</span> <span class="n">error</span><span class="p">,</span> <span class="n">test_predictions</span> <span class="o">=</span> <span class="n">cnn</span><span class="o">.</span><span class="n">get_error</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">test_d</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">test_l</span><span class="p">)</span>
<span class="k">print</span> <span class="s2">&quot;Test loss:&quot;</span><span class="p">,</span><span class="n">loss</span><span class="p">,</span> <span class="s2">&quot;Test error:&quot;</span><span class="p">,</span><span class="n">error</span><span class="o">*</span><span class="mf">100.</span><span class="p">,</span><span class="s2">&quot;%&quot;</span>
</pre></div>
</div>
<p>Of course the performance of this setup is not as good of the model above, but feel free tweak - how about dropout? Simply add <code class="docutils literal"><span class="pre">enable_dropout=True</span></code> to the cnn initialisation: all layers have by default a dropout rate of 0.5 - unless it is suppressed with <code class="docutils literal"><span class="pre">force_no_dropout=True</span></code> when adding a particular layer (it should not be used in the last layer). Don&#8217;t forget to set the dropout rates to 0 while estimating the performance and to their old value afterwards (the methods <code class="docutils literal"><span class="pre">cnn.getDropoutRates</span></code> and <code class="docutils literal"><span class="pre">cnn.setDropoutRates</span></code> might be useful). Hint: for dropout, a different activation function than <code class="docutils literal"><span class="pre">abs</span></code>, more neurons per layer and more training iterations might perform better... you can try adapting it yourself or find a ready setup with drop out in the <code class="docutils literal"><span class="pre">examples</span></code> folder.</p>
</div>
<div class="section" id="auto-encoder-example">
<span id="autoencoder"></span><h3><a class="toc-backref" href="#id18">Auto encoder Example</a><a class="headerlink" href="#auto-encoder-example" title="Permalink to this headline">¶</a></h3>
<p>This examples also uses MNIST data, but this time the task is not classification but compression. The input images have shape <code class="docutils literal"><span class="pre">28</span> <span class="pre">x</span> <span class="pre">28</span></code> but we will regard them as 784 dimensional vectors. The NN is shaped like an hourglass: the number of neurons decreases from 784 input neurons to 50 internal neurons in the central layer. Then the number increases symmetrically to 784 for the output. The training target is to reproduce the input in the output layer (i.e. the labels are identical to the data). Because the inputs are float numbers, so is the output and this is a regression problem. The first part of the auto encoder compresses the information and the second part decompresses it. The weights of both parts are shared, i.e. the weight matrix of each decompression layer is the transposed weight matrix of the corresponding compression layer, and updates are made simultaneously in both layers. For constructing an auto encoder the method <code class="docutils literal"><span class="pre">cnn.addTiedAutoencoderChain</span></code> is used.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">elektronn.training.traindata</span> <span class="kn">import</span> <span class="n">MNISTData</span>
<span class="kn">from</span> <span class="nn">elektronn.net.convnet</span> <span class="kn">import</span> <span class="n">MixedConvNN</span>
<span class="kn">from</span> <span class="nn">elektronn.net.introspection</span> <span class="kn">import</span> <span class="n">embedMatricesInGray</span>


<span class="c1"># Load Data #</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">MNISTData</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s1">&#39;/docs/devel/ELEKTRONN/elektronn/examples/mnist.pkl&#39;</span><span class="p">,</span><span class="n">convert2image</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">shift_augment</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>


<span class="c1"># Load Data #</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">MNISTData</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s1">&#39;~/devel/ELEKTRONN/Examples/mnist.pkl&#39;</span><span class="p">,</span><span class="n">convert2image</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">shift_augment</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Create Autoencoder #</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">cnn</span> <span class="o">=</span> <span class="n">MixedConvNN</span><span class="p">((</span><span class="mi">28</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span><span class="n">input_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">cnn</span><span class="o">.</span><span class="n">addPerceptronLayer</span><span class="p">(</span> <span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">300</span><span class="p">,</span> <span class="n">activation_func</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">)</span>
<span class="n">cnn</span><span class="o">.</span><span class="n">addPerceptronLayer</span><span class="p">(</span> <span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="n">activation_func</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">)</span>
<span class="n">cnn</span><span class="o">.</span><span class="n">addPerceptronLayer</span><span class="p">(</span> <span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">activation_func</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">)</span>
<span class="n">cnn</span><span class="o">.</span><span class="n">addTiedAutoencoderChain</span><span class="p">(</span><span class="n">n_layers</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">activation_func</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">,</span><span class="n">input_noise</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">add_layers_to_network</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">cnn</span><span class="o">.</span><span class="n">compileOutputFunctions</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="s2">&quot;regression&quot;</span><span class="p">)</span>  <span class="c1">#compiles the cnn.get_error function as well</span>
<span class="n">cnn</span><span class="o">.</span><span class="n">setOptimizerParams</span><span class="p">(</span><span class="n">SGD</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;LR&#39;</span><span class="p">:</span> <span class="mf">5e-1</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">},</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
  <span class="n">d</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">getbatch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
  <span class="n">loss</span><span class="p">,</span> <span class="n">loss_instance</span><span class="p">,</span> <span class="n">time_per_step</span> <span class="o">=</span> <span class="n">cnn</span><span class="o">.</span><span class="n">trainingStep</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;SGD&quot;</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="mi">100</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span> <span class="s2">&quot;update:&quot;</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="s2">&quot;; Training error:&quot;</span><span class="p">,</span><span class="n">loss</span>

<span class="n">loss</span><span class="p">,</span>  <span class="n">test_predictions</span> <span class="o">=</span> <span class="n">cnn</span><span class="o">.</span><span class="n">get_error</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">valid_d</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">valid_d</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">embedMatricesInGray</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">valid_d</span><span class="p">[:</span><span class="mi">200</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">200</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)),</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">recon</span> <span class="o">=</span> <span class="n">embedMatricesInGray</span><span class="p">(</span><span class="n">test_predictions</span><span class="p">[:</span><span class="mi">200</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">200</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)),</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">recon</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Reconstruction&#39;</span><span class="p">)</span>

<span class="n">cnn</span><span class="o">.</span><span class="n">saveParameters</span><span class="p">(</span><span class="s1">&#39;AE-pretraining.param&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The above NN learns to compress the 784 pixels of an image to a 50 dimensional code (ca. 15x). The quality of the reconstruction can be inspected from plotting the images and comparing them to the original input:</p>
<div class="figure align-center" id="id8">
<img alt="_images/DAE.png" src="_images/DAE.png" />
<p class="caption"><span class="caption-text">Left input data (from validation set) and right reconstruction. The reconstruction values have been slightly rescaled for better visualisation.</span></p>
</div>
<p>The compression part of the auto encoder can be used to reduce the dimension of a data vector, while still preserving the information necessary to reconstruct the original data.</p>
<p>Often training data (e.g. lots of images of digits) are vastly available but nobody has taken the effort to create training labels for all of them. This is when auto encoders can be useful: train an auto encoder on the unlabelled data and use the learnt weights to initialise a NN for classification (aka pre-training).The classifcation NN does not have to learn a good internal data representation from scratch. To fine-tune the weights for classification (mainly in the additional output layer), only a small fraction of the examples must be labelled. To construct a pre-trained NN:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">cnn</span><span class="o">.</span><span class="n">saveParameters</span><span class="p">(</span><span class="s1">&#39;AE-pretraining.param&#39;</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">cnn</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">])</span> <span class="c1"># save the parameters for the compression part</span>
<span class="n">cnn2</span> <span class="o">=</span> <span class="n">MixedConvNN</span><span class="p">((</span><span class="mi">28</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span><span class="n">input_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> <span class="c1"># Create a new NN</span>
<span class="n">cnn2</span><span class="o">.</span><span class="n">addPerceptronLayer</span><span class="p">(</span> <span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">300</span><span class="p">,</span> <span class="n">activation_func</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">)</span>
<span class="n">cnn2</span><span class="o">.</span><span class="n">addPerceptronLayer</span><span class="p">(</span> <span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="n">activation_func</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">)</span>
<span class="n">cnn2</span><span class="o">.</span><span class="n">addPerceptronLayer</span><span class="p">(</span> <span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">activation_func</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">)</span>
<span class="n">cnn2</span><span class="o">.</span><span class="n">addPerceptronLayer</span><span class="p">(</span> <span class="n">n_outputs</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">activation_func</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">)</span> <span class="c1"># Add a layer for 10-class classificaion</span>
<span class="n">cnn2</span><span class="o">.</span><span class="n">compileOutputFunctions</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="s2">&quot;nll&quot;</span><span class="p">)</span>  <span class="c1">#compiles the cnn.get_error function as well # target function nll for classification</span>
<span class="n">cnn2</span><span class="o">.</span><span class="n">setOptimizerParams</span><span class="p">(</span><span class="n">SGD</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;LR&#39;</span><span class="p">:</span> <span class="mf">0.005</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">},</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cnn2</span><span class="o">.</span><span class="n">loadParameters</span><span class="p">(</span><span class="s1">&#39;AE-pretraining.param&#39;</span><span class="p">)</span> <span class="c1"># This overloads only the first 3 layers,because the file contains only params for 3 layers</span>

<span class="c1"># Do training steps with the labels like</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
  <span class="n">d</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">getbatch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
  <span class="n">cnn2</span><span class="o">.</span><span class="n">trainingStep</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;SGD&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="IntroNN.html" title="Practical Introduction to Neural Networks"
             >next</a> |</li>
        <li class="right" >
          <a href="Installation.html" title="Installation"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">ELEKTRONN</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright Marius F Killinger, Gregor Urban, 2015.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.5.
    </div>
  </body>
</html>